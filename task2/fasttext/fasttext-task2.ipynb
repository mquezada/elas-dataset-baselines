{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2: clasificar conceptos y fundamentos abiertos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideramos los mismos modelos computados para la tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Carga los modelos creados previamente (OJO cargar los modelos de vectores preentrendos puede tardar mucho y usar mucha memoria porque pesa 1GB cada uno)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we assume models are already created\n",
    "# these are the global variables used when computing all the metrics\n",
    "# best models in with no pretrained vectors in _model_<tema>_best_at_<k>.bin\n",
    "# best models in with pretrained vectors in _model_<tema>_best_at_<k>.bin\n",
    "\n",
    "import fasttext\n",
    "\n",
    "_dataDir = \"../../data/tarea2/\"\n",
    "_models_task1 = \"../../task1/fasttext/\"\n",
    "ftlabel = \"__label__\"\n",
    "temas = [1,2,3,4]\n",
    "\n",
    "# prefix and suffixes for the models to load\n",
    "model_no_ptvec = \"_model_\"\n",
    "model_ptvec = \"_model_ptvec_\"\n",
    "at_1 = \"at_1\"\n",
    "at_5 = \"at_5\"\n",
    "\n",
    "# what models to consider\n",
    "# m_prefixes = [model_no_ptvec]\n",
    "# descomentar lo siguient si se quieren cargar los modelos con vectores preentrenados\n",
    "m_prefixes = [model_no_ptvec, model_ptvec]\n",
    "m_suffixes = [at_1,at_5]\n",
    "\n",
    "# names of all considered models, to iterate over models\n",
    "model_names = []\n",
    "for prefix in m_prefixes:\n",
    "    for suffix in m_suffixes:\n",
    "        model_names.append(prefix + suffix)\n",
    "        \n",
    "def print_model_name(model):\n",
    "    return model[7:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load models\n",
    "models = {}\n",
    "for prefix in m_prefixes:\n",
    "    for suffix in m_suffixes:\n",
    "        models[prefix + suffix] = {}\n",
    "        for tema in temas:\n",
    "            models[prefix + suffix][tema] =  fasttext.load_model(_models_task1 + prefix + str(tema) + \"_best_\" + suffix + \".bin\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Ahora carga los datos para ajustar los modelos. Cargamos solo test set</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data to predict\n",
    "\n",
    "import string\n",
    "\n",
    "def read_text_file_for_ft_input(filename):\n",
    "    with open(filename) as f:\n",
    "        out = []\n",
    "        for line in f:\n",
    "            # keep tab to separate original concepts from justifications\n",
    "            strdata = \"\".join([c for c in line[:-1] if c not in string.punctuation or c == '\\t']).lower()\n",
    "            if strdata == '':\n",
    "                strdata = ' '\n",
    "            out.append(strdata)\n",
    "    return out\n",
    "\n",
    "def read_numbers_file_for_ft_input(filename):\n",
    "    with open(filename) as f:\n",
    "        out = []\n",
    "        for line in f:\n",
    "            out.append(int(line))\n",
    "    return out\n",
    "\n",
    "test_x_concepto = {}\n",
    "test_x_fundamento = {}\n",
    "test_x_concepto_fundamento = {}\n",
    "test_y = {}\n",
    "\n",
    "for i in temas:\n",
    "    test_x_concepto_fundamento[i] = read_text_file_for_ft_input(\n",
    "        _dataDir + \"x_test_tema_\" + str(i) + \"_categorias_pnud_1.txt\")\n",
    "    test_y[i] = read_numbers_file_for_ft_input(\n",
    "        _dataDir + \"y_test_tema_\" + str(i) + \"_categorias_pnud_1.txt\")\n",
    "    \n",
    "for i in temas:\n",
    "    test_x_concepto[i] = []\n",
    "    test_x_fundamento[i] = []\n",
    "    for texto in test_x_concepto_fundamento[i]:\n",
    "        test_x_concepto[i].append(texto.split('\\t')[0])\n",
    "        test_x_fundamento[i].append(texto.split('\\t')[1])\n",
    "\n",
    "categories = {}\n",
    "for i in temas:\n",
    "    categories[i] = []\n",
    "    # load categories first\n",
    "    categoriesFile = _dataDir + \"categorias_tema_\" + str(i) + \"_pnud_0.txt\"\n",
    "    with open(categoriesFile) as f:\n",
    "        for line in f:\n",
    "            categories[i].append(line[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Computa las predicciones para cada tema y con cada uno de los modelos</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizes = {}\n",
    "sizes[1] = 37\n",
    "sizes[2] = 44\n",
    "sizes[3] = 12\n",
    "sizes[4] = 21\n",
    "\n",
    "predictions_with_labels = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    predictions_with_labels[model_name] = {}\n",
    "    for tema in temas:\n",
    "        predictions_with_labels[model_name][tema] = {}\n",
    "        predictions_with_labels[model_name][tema]['cf'] = models[model_name][tema].predict(\n",
    "            test_x_concepto_fundamento[tema],k = sizes[tema])\n",
    "        predictions_with_labels[model_name][tema]['c'] = models[model_name][tema].predict(\n",
    "            test_x_concepto[tema],k = sizes[tema])\n",
    "        predictions_with_labels[model_name][tema]['f'] = models[model_name][tema].predict(\n",
    "            test_x_fundamento[tema],k = sizes[tema])\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "# cambia las predicciones del tipo <ftlabel><C> a simplemente <C>\n",
    "for model_name in model_names:\n",
    "    predictions[model_name] = {}\n",
    "    for tema in temas:\n",
    "        predictions[model_name][tema] = {}\n",
    "        for inp in ['cf','c','f']:\n",
    "            predictions[model_name][tema][inp] = []\n",
    "            for pred_list_labels in predictions_with_labels[model_name][tema][inp]:\n",
    "                pred_list = []\n",
    "                for label in pred_list_labels:\n",
    "                    pred_list.append(int(label[len(ftlabel):]))\n",
    "                predictions[model_name][tema][inp].append(pred_list)\n",
    "            \n",
    "# utility function to select the first prediction from a list of predictions and generate a 1D list of single predictions\n",
    "def first_prediction(lists_of_predictions):\n",
    "    out = []\n",
    "    for predictions in lists_of_predictions:\n",
    "        out.append(predictions[0])\n",
    "    return out      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera reportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tema 1\t\tacc\trec\tprec\tf1\n",
      "npt-f\t\t46.12\t46.12\t45.09\t43.84\n",
      "npt-c\t\t61.31\t61.31\t65.47\t60.72\n",
      "npt-cf\t\t59.96\t59.96\t58.83\t57.26\n",
      "ptv-f\t\t47.91\t47.91\t47.27\t45.94\n",
      "ptv-c\t\t61.37\t61.37\t66.57\t60.7\n",
      "ptv-cf\t\t60.72\t60.72\t60.96\t58.38\n",
      "\n",
      "Tema 2\t\tacc\trec\tprec\tf1\n",
      "npt-f\t\t54.98\t54.98\t58.03\t54.11\n",
      "npt-c\t\t71.68\t71.68\t75.46\t71.34\n",
      "npt-cf\t\t69.56\t69.56\t70.21\t67.77\n",
      "ptv-f\t\t57.77\t57.77\t60.57\t57.03\n",
      "ptv-c\t\t73.31\t73.31\t76.17\t72.9\n",
      "ptv-cf\t\t70.62\t70.62\t72.79\t69.06\n",
      "\n",
      "Tema 3\t\tacc\trec\tprec\tf1\n",
      "npt-f\t\t60.19\t60.19\t65.7\t61.55\n",
      "npt-c\t\t77.31\t77.31\t81.86\t78.52\n",
      "npt-cf\t\t75.32\t75.32\t78.68\t75.62\n",
      "ptv-f\t\t61.65\t61.65\t66.8\t62.89\n",
      "ptv-c\t\t79.01\t79.01\t83.48\t80.17\n",
      "ptv-cf\t\t75.5\t75.5\t79.27\t75.94\n",
      "\n",
      "Tema 4\t\tacc\trec\tprec\tf1\n",
      "npt-f\t\t35.93\t35.93\t59.73\t32.37\n",
      "npt-c\t\t52.11\t52.11\t74.14\t49.74\n",
      "npt-cf\t\t49.7\t49.7\t69.79\t45.09\n",
      "ptv-f\t\t37.35\t37.35\t59.4\t34.25\n",
      "ptv-c\t\t55.29\t55.29\t76.51\t54.27\n",
      "ptv-cf\t\t52.71\t52.71\t72.44\t49.79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jperez/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/jperez/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/jperez/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/jperez/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for i in temas:\n",
    "    print(\"Tema \" + str(i) + \"\\t\\tacc\\trec\\tprec\\tf1\")\n",
    "    for model,model_name in zip(['_model_at_1', '_model_ptvec_at_1'],['npt','ptv']):\n",
    "        for inp in ['f','c','cf']:\n",
    "            prediction = first_prediction(predictions[model][i][inp])\n",
    "            acc = round(100*metrics.accuracy_score(test_y[i],prediction),2)\n",
    "            rec = round(100*metrics.recall_score(test_y[i],prediction,average='weighted'),2)\n",
    "            prec = round(100*metrics.precision_score(test_y[i],prediction,average='weighted'),2)\n",
    "            f1 = round(100*metrics.f1_score(test_y[i],prediction,average='weighted'),2)\n",
    "            print(model_name + \"-\" + inp + \"\\t\\t\"+ str(acc)+ \"\\t\"+ str(rec) +\"\\t\"+str(prec)+\"\\t\"+str(f1))\n",
    "    print()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera reporte para predicciones en listas (top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to compute top k accuracy\n",
    "\n",
    "def top_k_accuracy(gold,predicted,k):\n",
    "    '''\n",
    "    #Arguments\n",
    "        gold: the true labels of the test cases (size N = number of test cases)\n",
    "        predicted: ranked list of label predictions for every test case (size N x L, where L is assumed to be >= k)\n",
    "        k: the number of elements in the predicted lists that should be considered to compute the metric\n",
    "    #Returns\n",
    "        The portion of cases (between 0 and 1) in which the true label value was among the first k predicted labels\n",
    "    '''\n",
    "    count = 0\n",
    "    for g,pred_labels in zip(gold,predicted):\n",
    "        if g in pred_labels[:k]:\n",
    "                count += 1\n",
    "    return count/len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tema 1\t\t1\t3\t5\t7\t10\n",
      "test-ptv-f\t47.64\t70.21\t80.2\t84.64\t89.09\n",
      "test-ptv-c\t63.27\t85.3\t88.99\t92.3\t93.27\n",
      "test-ptv-cf\t60.72\t83.72\t89.91\t92.4\t94.63\n",
      "test-npt-f\t45.96\t68.53\t76.94\t81.82\t87.3\n",
      "test-npt-c\t60.12\t78.24\t83.18\t86.65\t87.68\n",
      "test-npt-cf\t59.74\t81.33\t87.52\t90.94\t93.71\n",
      "\n",
      "Tema 2\t\t1\t3\t5\t7\t10\n",
      "test-ptv-f\t57.71\t77.78\t83.62\t86.77\t89.95\n",
      "test-ptv-c\t72.77\t89.24\t91.81\t93.1\t95.12\n",
      "test-ptv-cf\t70.94\t88.54\t92.32\t94.12\t95.66\n",
      "test-npt-f\t55.2\t74.21\t81.31\t84.84\t87.99\n",
      "test-npt-c\t71.32\t87.64\t90.3\t91.78\t93.48\n",
      "test-npt-cf\t69.43\t87.12\t91.23\t92.68\t94.51\n",
      "\n",
      "Tema 3\t\t1\t3\t5\t7\t10\n",
      "test-ptv-f\t61.28\t83.12\t90.5\t94.19\t98.79\n",
      "test-ptv-c\t78.71\t92.14\t95.28\t96.91\t98.91\n",
      "test-ptv-cf\t75.5\t91.11\t95.52\t97.34\t99.58\n",
      "test-npt-f\t60.56\t81.55\t90.44\t95.04\t98.61\n",
      "test-npt-c\t77.13\t90.32\t94.13\t96.55\t98.73\n",
      "test-npt-cf\t75.62\t91.29\t95.52\t98.0\t99.46\n",
      "\n",
      "Tema 4\t\t1\t3\t5\t7\t10\n",
      "test-ptv-f\t37.81\t57.88\t68.92\t77.24\t85.27\n",
      "test-ptv-c\t55.68\t77.81\t86.44\t88.88\t90.44\n",
      "test-ptv-cf\t52.74\t73.38\t83.15\t89.42\t94.05\n",
      "test-npt-f\t35.93\t54.41\t64.88\t72.88\t82.65\n",
      "test-npt-c\t52.46\t64.32\t77.45\t81.24\t84.14\n",
      "test-npt-cf\t49.31\t65.45\t75.19\t82.65\t89.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_options = [1,3,5,7,10]\n",
    "for i in temas:\n",
    "    head_str = \"Tema \" + str(i) + \"\\t\\t\"\n",
    "    head_str += \"\\t\".join([str(op) for op in top_k_options])\n",
    "    print(head_str)\n",
    "    for model, model_name in zip(['_model_ptvec_at_5','_model_at_5'],['ptv','npt']):\n",
    "        for prediction,gold,set_name in zip(\n",
    "                [predictions[model][i]],\n",
    "                [test_y[i]],\n",
    "                ['test']):\n",
    "            for inp in ['f','c','cf']:\n",
    "                data_str = set_name + \"-\" + model_name + \"-\" + inp\n",
    "                for k in top_k_options:\n",
    "                    top_k = round(100*top_k_accuracy(gold,prediction[inp],k),2)\n",
    "                    data_str += \"\\t\" + str(top_k)\n",
    "                print(data_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ranking_sizes(gold,predicted):\n",
    "    r_sizes = []\n",
    "    for g,pred_labels in zip(gold,predicted):\n",
    "        if g not in pred_labels:\n",
    "            raise Exception('Label ' + str(g) + ' is not in the ranking.')\n",
    "        r_sizes.append(pred_labels.index(g) + 1)\n",
    "    return np.array(r_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tema 1\t\t80%\t85%\t90%\t95%\n",
      "real-ptv-f\t5\t8\t11\t20\n",
      "real-ptv-c\t3\t3\t6\t16\n",
      "real-ptv-cf\t3\t4\t6\t11\n",
      "real-npt-f\t7\t9\t13\t21\n",
      "real-npt-c\t4\t6\t12\t24\n",
      "real-npt-cf\t3\t4\t7\t12\n",
      "\n",
      "Tema 2\t\t80%\t85%\t90%\t95%\n",
      "real-ptv-f\t4\t6\t11\t21\n",
      "real-ptv-c\t2\t3\t4\t10\n",
      "real-ptv-cf\t2\t3\t4\t9\n",
      "real-npt-f\t5\t8\t13\t23\n",
      "real-npt-c\t2\t3\t5\t17\n",
      "real-npt-cf\t2\t3\t5\t11\n",
      "\n",
      "Tema 3\t\t80%\t85%\t90%\t95%\n",
      "real-ptv-f\t3\t4\t5\t8\n",
      "real-ptv-c\t2\t2\t3\t5\n",
      "real-ptv-cf\t2\t2\t3\t5\n",
      "real-npt-f\t3\t4\t5\t7\n",
      "real-npt-c\t2\t2\t3\t6\n",
      "real-npt-cf\t2\t2\t3\t5\n",
      "\n",
      "Tema 4\t\t80%\t85%\t90%\t95%\n",
      "real-ptv-f\t8\t10\t13\t17\n",
      "real-ptv-c\t4\t5\t9\t17\n",
      "real-ptv-cf\t5\t6\t8\t11\n",
      "real-npt-f\t9\t11\t15\t18\n",
      "real-npt-c\t7\t11\t17\t17\n",
      "real-npt-cf\t7\t8\t11\t17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentile_options = [80,85,90,95]\n",
    "for i in temas:\n",
    "    head_str = \"Tema \" + str(i) + \"\\t\\t\"\n",
    "    head_str += \"\\t\".join([str(op)+\"%\" for op in percentile_options])\n",
    "    print(head_str)\n",
    "    for model, model_name in zip(['_model_ptvec_at_5','_model_at_5'],['ptv','npt']):\n",
    "        prediction = predictions[model][i]\n",
    "        gold = test_y[i]\n",
    "        set_name = \"real\"\n",
    "        for inp in ['f','c','cf']:\n",
    "            data_str = set_name + \"-\" + model_name + \"-\" + inp\n",
    "            for k in percentile_options:\n",
    "                percentile = int(np.percentile(ranking_sizes(gold,prediction[inp]),k))\n",
    "                data_str += \"\\t\" + str(percentile)\n",
    "            print(data_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
