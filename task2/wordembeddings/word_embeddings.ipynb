{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data to predict\n",
    "_dataDir = \"../../data/tarea2/\"\n",
    "import string\n",
    "\n",
    "def read_text_file_for_ft_input(filename):\n",
    "    with open(filename) as f:\n",
    "        out = []\n",
    "        for line in f:\n",
    "            # keep tab to separate original concepts from justifications\n",
    "            strdata = \"\".join([c for c in line[:-1] if c not in string.punctuation or c == '\\t']).lower()\n",
    "            if strdata == '':\n",
    "                strdata = ' '\n",
    "            out.append(strdata)\n",
    "    return out\n",
    "\n",
    "def read_numbers_file_for_ft_input(filename):\n",
    "    with open(filename) as f:\n",
    "        out = []\n",
    "        for line in f:\n",
    "            out.append(int(line))\n",
    "    return out\n",
    "\n",
    "\n",
    "temas = [1,2,3,4]\n",
    "\n",
    "test_x_concepto = {}\n",
    "test_x_fundamento = {}\n",
    "test_x_concepto_fundamento = {}\n",
    "test_y = {}\n",
    "\n",
    "for i in temas:\n",
    "    test_x_concepto_fundamento[i] = read_text_file_for_ft_input(\n",
    "        _dataDir + \"x_test_tema_\" + str(i) + \"_categorias_pnud_1.txt\")\n",
    "    test_y[i] = read_numbers_file_for_ft_input(\n",
    "        _dataDir + \"y_test_tema_\" + str(i) + \"_categorias_pnud_1.txt\")\n",
    "    \n",
    "for i in temas:\n",
    "    test_x_concepto[i] = []\n",
    "    test_x_fundamento[i] = []\n",
    "    for texto in test_x_concepto_fundamento[i]:\n",
    "        test_x_concepto[i].append(texto.split('\\t')[0])\n",
    "        test_x_fundamento[i].append(texto.split('\\t')[1])\n",
    "\n",
    "categories = {}\n",
    "clean_categories = {}\n",
    "for i in temas:\n",
    "    categories[i] = []\n",
    "    clean_categories[i] = []\n",
    "    # load categories first\n",
    "    categoriesFile = _dataDir + \"categorias_tema_\" + str(i) + \"_pnud_0.txt\"\n",
    "    with open(categoriesFile) as f:\n",
    "        for line in f:\n",
    "            categories[i].append(line[:-1])\n",
    "            clean_categories[i].append(\"\".join([c for c in line[:-1] if c not in string.punctuation]).lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word  Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "### lo siguiente dejó de funcionar sin razón aparente (posiblemente luego de actualización de OS)\n",
    "### issue similara acá https://github.com/RaRe-Technologies/gensim/issues/1196\n",
    "# model = FastText.load_fasttext_format('../../word_vectors/wiki.es')\n",
    "\n",
    "### cargándolo con word2vec\n",
    "### carga solo los wordembeddings de palabras de los textos de la constitución\n",
    "model = FastText.load_word2vec_format('../../word_vectors/ca-vectors.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### esta es la forma de hacerlo con FastText model\n",
    "#index2word_set = set(model.wv.index2word)\n",
    "\n",
    "### mientras se use word2vec format se debe hacer así\n",
    "index2word_set = set(model.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.metrics import distance\n",
    "\n",
    "def avg_feature_vector(words, model, num_features, index2word_set):\n",
    "        #function to average all words vectors in a given paragraph\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "\n",
    "        #list containing names of words in the vocabulary\n",
    "        #index2word_set = set(model.index2word) this is moved as input param for performance reasons\n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                nwords = nwords+1\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "        if(nwords>0):\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def similarity(string1,string2):\n",
    "    vec1 = avg_feature_vector(string1.split(), model=model, num_features=300, index2word_set=index2word_set)\n",
    "    vec2 = avg_feature_vector(string2.split(), model=model, num_features=300, index2word_set=index2word_set)\n",
    "    return 1 - spatial.distance.cosine(vec1,vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings con los strings de los conceptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "we_prediction = {}\n",
    "\n",
    "for i in temas:\n",
    "    we_prediction[i] = []\n",
    "    for case in test_x_concepto[i]:\n",
    "        ### calcula la categoría mas similar\n",
    "        ranking_we = sorted(range(len(categories[i])), \n",
    "            key=lambda k: similarity(case,clean_categories[i][k]), reverse=True)\n",
    "        we_prediction[i].append(ranking_we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To clasify:\thacerse cargo de las propuestas\n",
      "Manual class.:\tresponsabilidad\n",
      "Word_embedding:\tde satisfacer cargas públicas\n"
     ]
    }
   ],
   "source": [
    "t_ex = 3\n",
    "c_ex = 1650\n",
    "\n",
    "print(\"To clasify:\\t\" + test_x_concepto[t_ex][c_ex])\n",
    "print(\"Manual class.:\\t\" + clean_categories[t_ex][test_y[t_ex][c_ex]])\n",
    "print(\"Word_embedding:\\t\" + clean_categories[t_ex][we_prediction[t_ex][c_ex][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings con los strings de los conceptos más los fundamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "we_ca_prediction = {}\n",
    "\n",
    "for i in temas:\n",
    "    we_ca_prediction[i] = []\n",
    "    for case_concepto,case_fundamento in zip(test_x_concepto[i],test_x_fundamento[i]):\n",
    "        ### calcula la categoría mas similar\n",
    "        ranking_we = sorted(range(len(categories[i])), \n",
    "            key=lambda k: similarity(\" \".join([case_concepto,case_fundamento]),clean_categories[i][k]), reverse=True)\n",
    "        we_ca_prediction[i].append(ranking_we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To clasify:\tdemocracia paritaria y proporcional en la toma de poder y decisiones debe existir igualdad de género para la discusión publica la legislación y las políticas públicas\n",
      "Manual class.:\tequidad de género\n",
      "Word_embedding:\trespeto  conservación de la naturaleza o medio ambiente\n"
     ]
    }
   ],
   "source": [
    "t_ex = 1\n",
    "c_ex = 700\n",
    "\n",
    "print(\"To clasify:\\t\" + \" \".join([test_x_concepto[t_ex][c_ex],test_x_fundamento[t_ex][c_ex]]))\n",
    "print(\"Manual class.:\\t\" + clean_categories[t_ex][test_y[t_ex][c_ex]])\n",
    "print(\"Word_embedding:\\t\" + clean_categories[t_ex][we_ca_prediction[t_ex][c_ex][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings con los strings de conceptos con peso por frecuencia inversa y quitando componente principal\n",
    "\n",
    "La componente principal se quita para los conjuntos de oraciones de cada tema después de normalizar por frecuencias. La constante de weighting es considerada (en general) como 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcula probabilidades y frecuencias usando el texto completo de los arugmentos constitucionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# primero computa tokes, vocabulario y frecuencias\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from collections import Counter\n",
    "\n",
    "_text_data_file = '../../data/texto_ca.txt'\n",
    "\n",
    "all_text = read_text_file_for_ft_input(_text_data_file)\n",
    "\n",
    "long_text = \" \".join(all_text)\n",
    "tokens = long_text.split()\n",
    "frequencies = Counter(tokens)\n",
    "probabilities = {}\n",
    "for word in set(tokens):\n",
    "    probabilities[word] = frequencies[word] / len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len((tokens)))\n",
    "#print(len(set(tokens)))\n",
    "#print(len(index2word_set))\n",
    "#print(len(frequencies))\n",
    "\n",
    "#not_in_voc = [w for w in set(tokens) if w not in index2word_set]\n",
    "#in_voc = [w for w in set(tokens) if w in index2word_set]\n",
    "\n",
    "#total_not_in_voc = len(not_in_voc)\n",
    "#total_in_voc = len(in_voc)\n",
    "\n",
    "#print(total_not_in_voc)\n",
    "#print(total_in_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#voc_out_file = '../../word_vectors/ca-vectors.voc'\n",
    "#with open(voc_out_file,'w') as f:\n",
    "#    for word,_ in frequencies.most_common():\n",
    "#        f.write(word)\n",
    "#        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### almacena vectores en word2vec fromat solo para las palabras que aparecen en los textos de la constitución\n",
    "###### el formato de los archivos comienza con el tamaño del vocabulario más la dimensión de los vectores\n",
    "#vectors_out_file = '../../word_vectors/ca-vectors.vec'\n",
    "\n",
    "#first_line = str(len(frequencies)) + \" \" + str(300) + \"\\n\"\n",
    "\n",
    "#with open(vectors_out_file) as f:\n",
    "#    f.write(first_line)\n",
    "#    for word,_ in frequencies.most_common():\n",
    "#        f.write(word + \" \" + str())\n",
    "#return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computa los embeddings con peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# funcion auxiliar para calcular el embedding normalizado por frecuencia para cada posible palabra\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def avg_feature_vector_weighted(words, model, num_features, index2word_set, word_probabilities, weighting_parameter=0.001):\n",
    "        #function to average all words vectors in a given paragraph\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                nwords = nwords+1\n",
    "                to_add = np.multiply(\n",
    "                    weighting_parameter / (weighting_parameter + word_probabilities[word]), \n",
    "                    model[word])\n",
    "                featureVec = np.add(featureVec, to_add)\n",
    "\n",
    "        if(nwords>0):\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_predictions_from_embeddings(classes, to_classify_set, distance):\n",
    "    predictions = []\n",
    "    \n",
    "    for case in to_classify_set:\n",
    "        ### calcula las predicciones por categorías mas similares\n",
    "    \n",
    "        ranking_weighted = sorted(\n",
    "            range(len(classes)),\n",
    "            key=lambda k: distance(case, classes[k])\n",
    "        )\n",
    "\n",
    "        predictions.append(ranking_weighted)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temas = [1,2,3,4]\n",
    "alpha = 0.001\n",
    "\n",
    "normalized_embeddings = {}\n",
    "embeddings = {}\n",
    "original_concepts_normalized_embeddings = {}\n",
    "original_concepts_embeddings = {}\n",
    "\n",
    "\n",
    "for i in temas:    \n",
    " \n",
    "    embeddings[i] = [\n",
    "        avg_feature_vector(sentence.split(), model, 300, index2word_set)\n",
    "        for sentence in test_x_concepto[i]\n",
    "    ]\n",
    "    \n",
    "    normalized_embeddings[i] = [\n",
    "        avg_feature_vector_weighted(sentence.split(), model, 300, index2word_set, probabilities, alpha)\n",
    "        for sentence in test_x_concepto[i]\n",
    "    ]\n",
    "    \n",
    "    original_concepts_embeddings[i] = [\n",
    "        avg_feature_vector(sentence.split(), model, 300, index2word_set)\n",
    "        for sentence in clean_categories[i]\n",
    "    ]\n",
    "    \n",
    "    original_concepts_normalized_embeddings[i] = [\n",
    "        avg_feature_vector_weighted(sentence.split(), model, 300, index2word_set, probabilities, alpha)\n",
    "        for sentence in clean_categories[i]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcula predicciones solo usando el weightening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighted_prediction = {}\n",
    "\n",
    "for i in temas:\n",
    "    # computa las predicciones\n",
    "    weighted_prediction[i] = compute_predictions_from_embeddings(\n",
    "        original_concepts_normalized_embeddings[i], normalized_embeddings[i], spatial.distance.cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To clasify:\thacerse cargo de las propuestas\n",
      "Manual class.:\tresponsabilidad\n",
      "Word_embedding_weighted:\tde satisfacer cargas públicas\n"
     ]
    }
   ],
   "source": [
    "t_ex = 3\n",
    "c_ex = 1650\n",
    "\n",
    "print(\"To clasify:\\t\" + \" \".join([test_x_concepto[t_ex][c_ex]]))\n",
    "print(\"Manual class.:\\t\" + clean_categories[t_ex][test_y[t_ex][c_ex]])\n",
    "print(\"Word_embedding_weighted:\\t\" + clean_categories[t_ex][weighted_prediction[t_ex][c_ex][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computa los PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = {}\n",
    "pca_weight = {}\n",
    "\n",
    "for i in temas:\n",
    "    # compute PCAs for embeddings\n",
    "    pca[i] = PCA(n_components=1).fit(\n",
    "        np.concatenate((embeddings[i], original_concepts_embeddings[i]))\n",
    "    ).components_[0]\n",
    "    \n",
    "    pca_weight[i] = PCA(n_components=1).fit(\n",
    "        np.concatenate((normalized_embeddings[i], original_concepts_normalized_embeddings[i]))\n",
    "    ).components_[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = pca[1][None].T\n",
    "M = u @ u.T\n",
    "R = M @ u\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcula predicciones usando el PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# primero computa los nuevos embeddings\n",
    "pca_embeddings = {}\n",
    "pca_original_concepts_embeddings = {}\n",
    "pca_weighted_embeddings = {}\n",
    "pca_original_concepts_normalized_embeddings = {}\n",
    "\n",
    "for i in temas:\n",
    "    \n",
    "    u = pca[i][None].T\n",
    "    M = u @ u.T\n",
    "    \n",
    "    pca_embeddings[i] = [\n",
    "        v - M @ v \n",
    "        for v in embeddings[i]\n",
    "    ]\n",
    "    \n",
    "    pca_original_concepts_embeddings[i] = [\n",
    "        v - M @ v\n",
    "        for v in original_concepts_embeddings[i]\n",
    "    ]\n",
    "    \n",
    "    u = pca_weight[i][None].T\n",
    "    M = u @ u.T\n",
    "\n",
    "    pca_weighted_embeddings[i] = [\n",
    "        v - M @ v\n",
    "        for v in normalized_embeddings[i]\n",
    "    ]\n",
    "    \n",
    "    pca_original_concepts_normalized_embeddings[i] = [\n",
    "        v - M @ v\n",
    "        for v in original_concepts_normalized_embeddings[i]\n",
    "    ]\n",
    "\n",
    "pca_prediction = {}\n",
    "pca_weighted_prediction = {}\n",
    "\n",
    "for i in temas:\n",
    "        \n",
    "    # computa las predicciones\n",
    "    pca_prediction[i] = compute_predictions_from_embeddings(\n",
    "        pca_original_concepts_embeddings[i], pca_embeddings[i], spatial.distance.cosine)\n",
    "    \n",
    "    pca_weighted_prediction[i] = compute_predictions_from_embeddings(\n",
    "        pca_original_concepts_normalized_embeddings[i], pca_weighted_embeddings[i], spatial.distance.cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To clasify:\tdemocracia paritaria y proporcional\n",
      "Manual class.:\tequidad de género\n",
      "Word_embedding_PCA_Weight:\tdemocracia\n"
     ]
    }
   ],
   "source": [
    "t_ex = 1\n",
    "c_ex = 700\n",
    "\n",
    "print(\"To clasify:\\t\" + \" \".join([test_x_concepto[t_ex][c_ex]]))\n",
    "print(\"Manual class.:\\t\" + clean_categories[t_ex][test_y[t_ex][c_ex]])\n",
    "#print(\"Word_embedding_PCA:\\t\" + clean_categories[t_ex][pca_prediction[t_ex][c_ex][0]])\n",
    "print(\"Word_embedding_PCA_Weight:\\t\" + clean_categories[t_ex][pca_weighted_prediction[t_ex][c_ex][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Edit Distance baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "edit_prediction = {}\n",
    "\n",
    "for i in temas:\n",
    "    edit_prediction[i] = [] \n",
    "    for case in test_x_concepto[i]:\n",
    "        ### calcula la categoría mas similar\n",
    "        ranking_edit = sorted(range(len(categories[i])), \n",
    "            key=lambda k: editdistance.eval(case,clean_categories[i][k]))\n",
    "        edit_prediction[i].append(ranking_edit)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To clasify:\thacerse cargo de las propuestas\n",
      "Manual class.:\tresponsabilidad\n",
      "Edit distance:\tservicio a la comunidad\n"
     ]
    }
   ],
   "source": [
    "t_ex = 3\n",
    "c_ex = 1650\n",
    "\n",
    "print(\"To clasify:\\t\" + test_x_concepto[t_ex][c_ex])\n",
    "print(\"Manual class.:\\t\" + clean_categories[t_ex][test_y[t_ex][c_ex]])\n",
    "print(\"Edit distance:\\t\" + clean_categories[t_ex][edit_prediction[t_ex][c_ex][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcula métricas\n",
    "\n",
    "En las listas iniciales (predictions y model_names) se deben indicar las predicciones que se utilizarán para calcular las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to compute top k accuracy\n",
    "\n",
    "def first_predictions(predictions_list):\n",
    "    return [li[0] for li in predictions_list]\n",
    "    \n",
    "\n",
    "def top_k_accuracy(gold,predicted,k):\n",
    "    '''\n",
    "    #Arguments\n",
    "        gold: the true labels of the test cases (size N = number of test cases)\n",
    "        predicted: ranked list of label predictions for every test case (size N x L, where L is assumed to be >= k)\n",
    "        k: the number of elements in the predicted lists that should be considered to compute the metric\n",
    "    #Returns\n",
    "        The portion of cases (between 0 and 1) in which the true label value was among the first k predicted labels\n",
    "    '''\n",
    "    count = 0\n",
    "    for g,pred_labels in zip(gold,predicted):\n",
    "        if g in pred_labels[:k]:\n",
    "                count += 1\n",
    "    return count/len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc, top-5\n",
      "Edit-distance $(c)$ & 41.2 & 60.6 & 30.9 & 46.7 & 41.6 & 64.9 & 22.7 & 38.6 &  & 34.1 & 52.7 \\\\ \n",
      "Word-embeddings $(c)$ & 60.2 & 86.3 & 58.8 & 79.1 & 60.4 & 80.8 & 45.5 & 86.1 &  & 56.2 & 83.1 \\\\ \n",
      "Word-embeddings $(c,a)$ & 18.4 & 62.6 & 29.7 & 64.7 & 47.5 & 67.8 & 46.1 & 76.3 &  & 35.4 & 67.9 \\\\ \n",
      "Weighted $(c)$ & 62.2 & 88.0 & 58.3 & 82.6 & 62.9 & 84.1 & 39.0 & 84.0 &  & 55.6 & 84.7 \\\\ \n",
      "PCA $(c)$ & 61.5 & 86.2 & 57.9 & 77.9 & 52.9 & 79.4 & 44.2 & 85.3 &  & 54.1 & 82.2 \\\\ \n",
      "Weighted-PCA $(c)$ & 67.2 & 89.4 & 60.6 & 83.1 & 58.6 & 81.5 & 40.0 & 86.6 &  & 56.6 & 85.2 \\\\ \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn import metrics\n",
    "\n",
    "# lista para promedios\n",
    "temas = [1,2,3,4]\n",
    "\n",
    "# lista de predicciones\n",
    "predictions = [\n",
    "    edit_prediction,\n",
    "    we_prediction,\n",
    "    we_ca_prediction,\n",
    "    weighted_prediction,\n",
    "    pca_prediction,\n",
    "    pca_weighted_prediction\n",
    "]\n",
    "\n",
    "# nombres de los modelos (solo para efectos de presentacion)\n",
    "model_names = [\n",
    "  'Edit-distance $(c)$',\n",
    "  'Word-embeddings $(c)$',\n",
    "  'Word-embeddings $(c,a)$',\n",
    "  'Weighted $(c)$',\n",
    "  'PCA $(c)$',\n",
    "  'Weighted-PCA $(c)$',\n",
    "]\n",
    "\n",
    "print(\"acc, top-5\")\n",
    "for prediction,model_name in zip(predictions,model_names):\n",
    "    sys.stdout.write(model_name + \" & \")\n",
    "    avga = 0\n",
    "    avgt = 0\n",
    "    for i in temas:\n",
    "        gold = test_y[i]\n",
    "        acc = round(100*metrics.accuracy_score(gold,first_predictions(prediction[i])),1)\n",
    "        avga += acc/len(temas)\n",
    "        top_k = round(100*top_k_accuracy(gold,prediction[i],5),1)\n",
    "        avgt += top_k/len(temas)\n",
    "        sys.stdout.write(format(acc,\"04.1f\") + \" & \" + format(top_k,\"04.1f\") + \" & \")\n",
    "    avga = round(avga,1)\n",
    "    avgt = round(avgt,1)\n",
    "    sys.stdout.write(\" & \" + \"%04.1f\" %(avga) + \" & \" + \"%04.1f\" %(avgt) + \" \\\\\\\\ \\n\")        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"top-5 per topic\")\n",
    "#print(temas)\n",
    "#for prediction,model_name in zip(predictions,model_names):\n",
    "#    sys.stdout.write(model_name + \" & \")\n",
    "#    avgt = 0\n",
    "#    for i in temas:\n",
    "#        gold = test_y[i]\n",
    "#        top_k = round(100*top_k_accuracy(gold,prediction[i],5),1)\n",
    "#        avgt += top_k/4\n",
    "#        sys.stdout.write(str(top_k) + \" & \")\n",
    "#    avgt = round(avgt,1)\n",
    "#    sys.stdout.write(str(avgt) + \" \\\\\\\\ \\n\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
