{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from itertools import product\n",
    "import unicodedata\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Dropout, Input, Embedding, Lambda\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for tema in range(0, 4):\n",
    "    dict_column = {}\n",
    "    for column in ['x', 'y']:\n",
    "        dict_set = {}\n",
    "        for set_ in ['train', 'test']:\n",
    "            filename = '../../data/tarea2/'+column+'_'+set_+'_tema_'+str(tema+1)+'_categorias_pnud_'\n",
    "            filename += '0.txt' if set_=='train' else '1.txt'\n",
    "            with open(filename) as f:\n",
    "                data = f.readlines()\n",
    "            dict_set[set_] = [row[:-1] for row in data]\n",
    "        dict_column[column] = dict_set\n",
    "    df.append(dict_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y', 'x']\n",
      "['test', 'train']\n"
     ]
    }
   ],
   "source": [
    "print df[0].keys()\n",
    "print df[0]['x'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Matrix with fasttext vectors for all words in the data set\n",
    "embedding_matrix = np.loadtxt('../../task1/DAN/dan_preprocessing_data/embedding_matrix.txt', dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conversion from string to int for all words in the data set\n",
    "word_to_num = pickle.load(open(\"../../task1/DAN/dan_preprocessing_data/dict_word_to_num.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300  # size fasttext vectors\n",
    "global encoder       # to detect number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_y_sets(NUM_DF):\n",
    "    TRAIN_SIZE = len(df[NUM_DF]['x']['train'])\n",
    "    TEST_SIZE = len(df[NUM_DF]['x']['test'])\n",
    "    df_y = np.array(df[NUM_DF]['y']['train'] + df[NUM_DF]['y']['test'])\n",
    "    # one hot vector label for clasification\n",
    "    global encoder\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_y) # to know how many classes \n",
    "    labels = encoder.transform(df_y)\n",
    "    Y = to_categorical(np.asarray(labels))\n",
    "    \n",
    "    y_train = Y[0 : TRAIN_SIZE]\n",
    "    y_test = Y[TRAIN_SIZE : ]\n",
    "    return y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global df_x, sequences, MAX_SEQUENCE_LENGTH\n",
    "df_x = [None, None, None, None]              # argumentos de train+dev+test para cada tema\n",
    "sequences = [None, None, None, None]         # argumentos como listas de palabras para cada tema\n",
    "MAX_SEQUENCE_LENGTH = 0                      # tamaño máximo de vector de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_x(num_df):\n",
    "    '''\n",
    "    Converts arguments in word sequence saving it in global sequences array \n",
    "    Updates the global max sequence length.\n",
    "    \n",
    "    Arguments:\n",
    "        num_df: theme number to process\n",
    "    '''\n",
    "    global MAX_SEQUENCE_LENGTH, df_x, sequences\n",
    "    df_x[num_df] = np.array(df[num_df]['x']['train'] + df[num_df]['x']['test'])\n",
    "    # to list of words\n",
    "    sequences[num_df] = []\n",
    "    for argument_j in range(0, df_x[num_df].shape[0]):\n",
    "        in_unicode = df_x[num_df][argument_j].decode('utf-8')\n",
    "        in_string = unicodedata.normalize('NFKD', in_unicode).encode('ascii','ignore')\n",
    "        if argument_j == 7291:\n",
    "            especial = text_to_word_sequence(in_string)\n",
    "        sequences[num_df].append(text_to_word_sequence(in_string))\n",
    "    # search for the biggest\n",
    "    for sequence in sequences[num_df]:\n",
    "        if len(sequence) > MAX_SEQUENCE_LENGTH:\n",
    "            MAX_SEQUENCE_LENGTH = len(sequence)\n",
    "\n",
    "def get_x_sets(num_df):\n",
    "    '''\n",
    "    Replaces word in sequences for corresponding numbers.\n",
    "    Arguments:\n",
    "        num_df: theme number from which to get the sets\n",
    "    Returns:\n",
    "        Train, development and test set\n",
    "    '''\n",
    "    global df_x, sequences\n",
    "    # every X[i] with max size\n",
    "    # replace words by numbers with world_dict\n",
    "    X = np.zeros((df_x[num_df].shape[0], MAX_SEQUENCE_LENGTH)).astype(int)\n",
    "    for i in range(0, len(sequences[num_df])):\n",
    "        for j in range(0, len(sequences[num_df][i])):\n",
    "            X[i][-len(sequences[num_df][i])+j] = word_to_num[sequences[num_df][i][j]]\n",
    "    # divide sets for answer\n",
    "    TRAIN_SIZE = len(df[num_df]['x']['train'])\n",
    "    TEST_SIZE = len(df[num_df]['x']['test'])\n",
    "    X_train = X[0 : TRAIN_SIZE]\n",
    "    X_test = X[TRAIN_SIZE : ]\n",
    "    return X_train, X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    preprocess_x(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['la',\n",
       " 'tecnologia',\n",
       " 'hoy',\n",
       " 'nos',\n",
       " 'permite',\n",
       " 'pensar',\n",
       " 'en',\n",
       " 'instrumentos',\n",
       " 'efectivos',\n",
       " 'de',\n",
       " 'participacion',\n",
       " 'directa']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train best configuration models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs: \n",
    "- DAN ocupa los sets globales  \n",
    "- como no hay set de validación el de entrenamiento es dividio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dan(relu_layers=3, hidden_units=300, p_dropout=0.3, dropout_input=False, \n",
    "        my_regularizer=regularizers.l2(1e-5), my_optimizer='adam', \n",
    "        epochs=150, batch_size=200):\n",
    "    '''\n",
    "    Creates and fit NN. \n",
    "    NN Arquitecture: \n",
    "        Input: vector with numbers representing index in embedding layer\n",
    "        Embedding layer: matrix multiplication to obtain vectors for each index in the input\n",
    "        Dropout: optional words dropout\n",
    "        Mean: Averages the vectors of the embedding output, returning one averaged vector\n",
    "        Fully connected: Fully connected layers with relu as activation function\n",
    "                         optional neuron dropout\n",
    "        Fully connected: output layer with softmax function\n",
    "    \n",
    "    Arguments:\n",
    "        relu_layers: Number of fully connected layers with relu \n",
    "        hidden_units: Number of neurons on the relu layers\n",
    "        p_dropout: dropout probability for the relu layers\n",
    "        dropout_input: dropout probability for words \n",
    "        my_regularizer: kernel_regularizer for fully connected layers\n",
    "        my_optimizer: optimizer for back propagation\n",
    "        epochs: number of epochs to train\n",
    "        batch_size: batch_size for trainning\n",
    "    '''\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "    x = Embedding(len(word_to_num) + 1, EMBEDDING_DIM, mask_zero=True,\n",
    "                  weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "    if dropout_input:\n",
    "        x = Dropout(0.5)(x)\n",
    "    x = Lambda(lambda x: K.mean(x, axis=1), \n",
    "               output_shape=(embedding_matrix.shape[1],))(x)\n",
    "    for i in range(0, relu_layers):\n",
    "        x = Dropout(p_dropout)(x)\n",
    "        x = Dense(units=hidden_units, activation='relu', kernel_regularizer=my_regularizer)(x)\n",
    "    preds = Dense(units=len(encoder.classes_), activation='softmax', \n",
    "                  kernel_regularizer=my_regularizer)(x)\n",
    "    \n",
    "    m = Model(sequence_input, preds)\n",
    "    m.compile(loss='categorical_crossentropy', optimizer=my_optimizer, \n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "    print \"Starting trainning\"\n",
    "    # use 20% as validation\n",
    "    m.fit(X_train, y_train, validation_split=0.2,\n",
    "          shuffle=True, \n",
    "          epochs=epochs, batch_size=batch_size,\n",
    "          verbose=1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models for each theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- TEMA 1--------------------------\n",
      "Starting trainning\n",
      "Train on 36886 samples, validate on 9222 samples\n",
      "Epoch 1/80\n",
      "36886/36886 [==============================] - 20s - loss: 2.3038 - acc: 0.2879 - top_k_categorical_accuracy: 0.6477 - val_loss: 12.6276 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/80\n",
      "36886/36886 [==============================] - 23s - loss: 1.6357 - acc: 0.4867 - top_k_categorical_accuracy: 0.8433 - val_loss: 13.6493 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0011\n",
      "Epoch 3/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4561 - acc: 0.5431 - top_k_categorical_accuracy: 0.8775 - val_loss: 13.9179 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0127\n",
      "Epoch 4/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.3573 - acc: 0.5757 - top_k_categorical_accuracy: 0.8896 - val_loss: 14.3310 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0306\n",
      "Epoch 5/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.2987 - acc: 0.5978 - top_k_categorical_accuracy: 0.8983 - val_loss: 14.6700 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0234\n",
      "Epoch 6/80\n",
      "36886/36886 [==============================] - 17s - loss: 1.2715 - acc: 0.6087 - top_k_categorical_accuracy: 0.8990 - val_loss: 14.6294 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0347\n",
      "Epoch 7/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.2383 - acc: 0.6189 - top_k_categorical_accuracy: 0.9057 - val_loss: 14.9539 - val_acc: 3.2531e-04 - val_top_k_categorical_accuracy: 0.0338\n",
      "Epoch 8/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.2225 - acc: 0.6248 - top_k_categorical_accuracy: 0.9072 - val_loss: 15.0203 - val_acc: 7.5905e-04 - val_top_k_categorical_accuracy: 0.0403\n",
      "Epoch 9/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.2054 - acc: 0.6295 - top_k_categorical_accuracy: 0.9112 - val_loss: 14.9533 - val_acc: 0.0068 - val_top_k_categorical_accuracy: 0.0449\n",
      "Epoch 10/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.1894 - acc: 0.6334 - top_k_categorical_accuracy: 0.9127 - val_loss: 14.9730 - val_acc: 0.0159 - val_top_k_categorical_accuracy: 0.0569\n",
      "Epoch 11/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.1853 - acc: 0.6367 - top_k_categorical_accuracy: 0.9114 - val_loss: 15.0290 - val_acc: 0.0127 - val_top_k_categorical_accuracy: 0.0471\n",
      "Epoch 12/80\n",
      "36886/36886 [==============================] - 23s - loss: 1.1684 - acc: 0.6395 - top_k_categorical_accuracy: 0.9160 - val_loss: 15.0097 - val_acc: 0.0217 - val_top_k_categorical_accuracy: 0.0548\n",
      "Epoch 13/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.1616 - acc: 0.6424 - top_k_categorical_accuracy: 0.9164 - val_loss: 14.9791 - val_acc: 0.0279 - val_top_k_categorical_accuracy: 0.0631\n",
      "Epoch 14/80\n",
      "36886/36886 [==============================] - 21s - loss: 1.1547 - acc: 0.6464 - top_k_categorical_accuracy: 0.9170 - val_loss: 15.0219 - val_acc: 0.0202 - val_top_k_categorical_accuracy: 0.0507\n",
      "Epoch 15/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.1478 - acc: 0.6467 - top_k_categorical_accuracy: 0.9180 - val_loss: 15.0186 - val_acc: 0.0187 - val_top_k_categorical_accuracy: 0.0520\n",
      "Epoch 16/80\n",
      "36886/36886 [==============================] - 21s - loss: 1.1427 - acc: 0.6475 - top_k_categorical_accuracy: 0.9184 - val_loss: 14.9869 - val_acc: 0.0300 - val_top_k_categorical_accuracy: 0.0592\n",
      "Epoch 17/80\n",
      "36886/36886 [==============================] - 22s - loss: 1.1330 - acc: 0.6523 - top_k_categorical_accuracy: 0.9180 - val_loss: 14.9994 - val_acc: 0.0257 - val_top_k_categorical_accuracy: 0.0589\n",
      "Epoch 18/80\n",
      "36886/36886 [==============================] - 21s - loss: 1.1295 - acc: 0.6531 - top_k_categorical_accuracy: 0.9198 - val_loss: 14.9640 - val_acc: 0.0377 - val_top_k_categorical_accuracy: 0.0640\n",
      "Epoch 19/80\n",
      "36886/36886 [==============================] - 21s - loss: 1.1261 - acc: 0.6534 - top_k_categorical_accuracy: 0.9202 - val_loss: 14.9841 - val_acc: 0.0298 - val_top_k_categorical_accuracy: 0.0605\n",
      "Epoch 20/80\n",
      "36886/36886 [==============================] - 21s - loss: 1.1197 - acc: 0.6562 - top_k_categorical_accuracy: 0.9224 - val_loss: 14.9554 - val_acc: 0.0394 - val_top_k_categorical_accuracy: 0.0663\n",
      "Epoch 21/80\n",
      "36886/36886 [==============================] - 23s - loss: 1.1157 - acc: 0.6556 - top_k_categorical_accuracy: 0.9209 - val_loss: 14.9967 - val_acc: 0.0310 - val_top_k_categorical_accuracy: 0.0570\n",
      "Epoch 22/80\n",
      "36886/36886 [==============================] - 21s - loss: 1.1161 - acc: 0.6565 - top_k_categorical_accuracy: 0.9219 - val_loss: 15.0085 - val_acc: 0.0278 - val_top_k_categorical_accuracy: 0.0538\n",
      "Epoch 23/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.1107 - acc: 0.6568 - top_k_categorical_accuracy: 0.9205 - val_loss: 14.9925 - val_acc: 0.0306 - val_top_k_categorical_accuracy: 0.0573\n",
      "Epoch 24/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.1091 - acc: 0.6552 - top_k_categorical_accuracy: 0.9221 - val_loss: 14.9754 - val_acc: 0.0372 - val_top_k_categorical_accuracy: 0.0596\n",
      "Epoch 25/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.1102 - acc: 0.6572 - top_k_categorical_accuracy: 0.9225 - val_loss: 14.9942 - val_acc: 0.0324 - val_top_k_categorical_accuracy: 0.0566\n",
      "Epoch 26/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.0967 - acc: 0.6629 - top_k_categorical_accuracy: 0.9257 - val_loss: 14.9607 - val_acc: 0.0406 - val_top_k_categorical_accuracy: 0.0632\n",
      "Epoch 27/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.0977 - acc: 0.6599 - top_k_categorical_accuracy: 0.9241 - val_loss: 14.9642 - val_acc: 0.0384 - val_top_k_categorical_accuracy: 0.0645\n",
      "Epoch 28/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.0917 - acc: 0.6611 - top_k_categorical_accuracy: 0.9252 - val_loss: 14.9811 - val_acc: 0.0354 - val_top_k_categorical_accuracy: 0.0594\n",
      "Epoch 29/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.0919 - acc: 0.6620 - top_k_categorical_accuracy: 0.9254 - val_loss: 14.9445 - val_acc: 0.0441 - val_top_k_categorical_accuracy: 0.0658\n",
      "Epoch 30/80\n",
      "36886/36886 [==============================] - 21s - loss: 1.0900 - acc: 0.6652 - top_k_categorical_accuracy: 0.9262 - val_loss: 14.9632 - val_acc: 0.0387 - val_top_k_categorical_accuracy: 0.0633\n",
      "Epoch 31/80\n",
      "36886/36886 [==============================] - 23s - loss: 1.0840 - acc: 0.6635 - top_k_categorical_accuracy: 0.9254 - val_loss: 14.9788 - val_acc: 0.0365 - val_top_k_categorical_accuracy: 0.0604\n",
      "Epoch 32/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.0832 - acc: 0.6628 - top_k_categorical_accuracy: 0.9256 - val_loss: 14.9925 - val_acc: 0.0338 - val_top_k_categorical_accuracy: 0.0577\n",
      "Epoch 33/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.0804 - acc: 0.6650 - top_k_categorical_accuracy: 0.9263 - val_loss: 14.9562 - val_acc: 0.0421 - val_top_k_categorical_accuracy: 0.0639\n",
      "Epoch 34/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.0788 - acc: 0.6667 - top_k_categorical_accuracy: 0.9277 - val_loss: 14.9470 - val_acc: 0.0439 - val_top_k_categorical_accuracy: 0.0661\n",
      "Epoch 35/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.0786 - acc: 0.6656 - top_k_categorical_accuracy: 0.9254 - val_loss: 14.9382 - val_acc: 0.0465 - val_top_k_categorical_accuracy: 0.0667\n",
      "Epoch 36/80\n",
      "36886/36886 [==============================] - 22s - loss: 1.0743 - acc: 0.6658 - top_k_categorical_accuracy: 0.9276 - val_loss: 14.9807 - val_acc: 0.0363 - val_top_k_categorical_accuracy: 0.0601\n",
      "Epoch 37/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.0698 - acc: 0.6686 - top_k_categorical_accuracy: 0.9286 - val_loss: 14.9354 - val_acc: 0.0460 - val_top_k_categorical_accuracy: 0.0668\n",
      "Epoch 38/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.0653 - acc: 0.6715 - top_k_categorical_accuracy: 0.9294 - val_loss: 14.9819 - val_acc: 0.0374 - val_top_k_categorical_accuracy: 0.0589\n",
      "Epoch 39/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.0687 - acc: 0.6683 - top_k_categorical_accuracy: 0.9302 - val_loss: 14.9798 - val_acc: 0.0381 - val_top_k_categorical_accuracy: 0.0594\n",
      "Epoch 40/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.0566 - acc: 0.6728 - top_k_categorical_accuracy: 0.9300 - val_loss: 14.9615 - val_acc: 0.0410 - val_top_k_categorical_accuracy: 0.0622\n",
      "Epoch 41/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.0632 - acc: 0.6681 - top_k_categorical_accuracy: 0.9284 - val_loss: 14.9485 - val_acc: 0.0436 - val_top_k_categorical_accuracy: 0.0647\n",
      "Epoch 42/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.0607 - acc: 0.6718 - top_k_categorical_accuracy: 0.9292 - val_loss: 14.9930 - val_acc: 0.0359 - val_top_k_categorical_accuracy: 0.0580\n",
      "Epoch 43/80\n",
      "31020/36886 [========================>.....] - ETA: 2s - loss: 1.0576 - acc: 0.6683 - top_k_categorical_accuracy: 0.9315"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1):\n",
    "    print \"----------------------- TEMA \"+str(i+1)+\"--------------------------\"\n",
    "    y_train, y_test = get_y_sets(i)\n",
    "    X_train, X_test = get_x_sets(i)\n",
    "    batch_size = 30\n",
    "    epochs = 80\n",
    "    l2 = None if i != 1 else regularizers.l2(1e-5) # the best has not regularizator in theme 2\n",
    "    m = dan(relu_layers=2, hidden_units=200, \n",
    "            p_dropout=0.2, my_regularizer=l2,\n",
    "            epochs=epochs, batch_size=batch_size)\n",
    "    m.save('models/tema'+str(i+1)+'.h5')\n",
    "    score = m.evaluate(X_train, y_train)\n",
    "    print \"\\nTRAIN:\"\n",
    "    print \"-\", m.metrics_names[0], score[0]\n",
    "    print \"-\", m.metrics_names[1], score[1]\n",
    "    print \"-\", m.metrics_names[2], score[2]\n",
    "    score = m.evaluate(X_test, y_test)\n",
    "    print \"\\nTEST:\"\n",
    "    print \"-\", m.metrics_names[0], score[0]\n",
    "    print \"-\", m.metrics_names[1], score[1]\n",
    "    print \"-\", m.metrics_names[2], score[2], '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- TEMA 1--------------------------\n",
      "46016/46108 [============================>.] - ETA: 0s\n",
      "TRAIN:\n",
      "- loss 1.1321175075\n",
      "- acc 0.672724906746\n",
      "- top_k_categorical_accuracy 0.915741303012\n",
      "1792/1843 [============================>.] - ETA: 0s\n",
      "TEST:\n",
      "- loss 1.38201149446\n",
      "- acc 0.609875203602\n",
      "- top_k_categorical_accuracy 0.895279435703 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1):\n",
    "    print \"----------------------- TEMA \"+str(i+1)+\"--------------------------\"\n",
    "    y_train, y_test = get_y_sets(i)\n",
    "    X_train, X_test = get_x_sets(i)\n",
    "    m = load_model(\"../../task1/DAN/models/tema\"+str(i+1)+'.h5')\n",
    "    score = m.evaluate(X_train, y_train)\n",
    "    print \"\\nTRAIN:\"\n",
    "    print \"-\", m.metrics_names[0], score[0]\n",
    "    print \"-\", m.metrics_names[1], score[1]\n",
    "    print \"-\", m.metrics_names[2], score[2]\n",
    "    score = m.evaluate(X_test, y_test)\n",
    "    print \"\\nTEST:\"\n",
    "    print \"-\", m.metrics_names[0], score[0]\n",
    "    print \"-\", m.metrics_names[1], score[1]\n",
    "    print \"-\", m.metrics_names[2], score[2], '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
