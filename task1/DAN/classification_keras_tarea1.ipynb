{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1\n",
    "Se cargan los datos correspondientes a la tarea 1, la matriz de pesos extraídos de fasttext (embedding layer) y el diccionario de palabras: int. Las id de las palabras en ese diccionario representan el índice en donde se encuentra el vector de esa palabra en el embedding layer.\n",
    "\n",
    "Los identificadores de labels (conceptos) son convertidos a one hot vector\n",
    "\n",
    "La función preprocess_x tiene como objetivo encontrar la secuencia de palabras (argumento) más largo, de tal forma de luego hacer padding a las entradas más pequeñas. Para no tener errores probando otras entradas más largas, se aumentó este valor en 100 para tener rango de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from itertools import product\n",
    "import unicodedata\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Dropout, Input, Embedding, Lambda\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for tema in range(0, 4):\n",
    "    dict_column = {}\n",
    "    for column in ['x', 'y']:\n",
    "        dict_set = {}\n",
    "        for set_ in ['train', 'dev', 'test']:\n",
    "            filename = '../../data/'+column+'_'+set_+'_tema_'+str(tema+1)+'_categorias_pnud_0.txt'\n",
    "            with open(filename) as f:\n",
    "                data = f.readlines()\n",
    "            dict_set[set_] = [row[:-1] for row in data]\n",
    "        dict_column[column] = dict_set\n",
    "    df.append(dict_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y', 'x']\n",
      "['test', 'train', 'dev']\n"
     ]
    }
   ],
   "source": [
    "print df[0].keys()\n",
    "print df[0]['x'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Matrix with fasttext vectors for all words in the data set\n",
    "embedding_matrix = np.loadtxt('dan_preprocessing_data/embedding_matrix.txt', dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conversion from string to int for all words in the data set\n",
    "word_to_num = pickle.load(open(\"dan_preprocessing_data/dict_word_to_num.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300  # size fasttext vectors\n",
    "global encoder       # to detect number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_y_sets(NUM_DF):\n",
    "    TRAIN_SIZE = len(df[NUM_DF]['x']['train'])\n",
    "    DEV_SIZE = len(df[NUM_DF]['x']['dev'])\n",
    "    TEST_SIZE = len(df[NUM_DF]['x']['test'])\n",
    "    df_y = np.array(df[NUM_DF]['y']['train'] + df[NUM_DF]['y']['dev'] + df[NUM_DF]['y']['test'])\n",
    "    # one hot vector label for clasification\n",
    "    global encoder\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_y) # to know how many classes \n",
    "    labels = encoder.transform(df_y)\n",
    "    Y = to_categorical(np.asarray(labels))\n",
    "    \n",
    "    y_train = Y[0 : TRAIN_SIZE]\n",
    "    y_dev = Y[TRAIN_SIZE : TRAIN_SIZE+DEV_SIZE]\n",
    "    y_test = Y[TRAIN_SIZE+DEV_SIZE : ]\n",
    "    return y_train, y_dev, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global df_x, sequences, MAX_SEQUENCE_LENGTH\n",
    "df_x = [None, None, None, None]              # argumentos de train+dev+test para cada tema\n",
    "sequences = [None, None, None, None]         # argumentos como listas de palabras para cada tema\n",
    "MAX_SEQUENCE_LENGTH = 0                      # tamaño máximo de vector de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_x(num_df):\n",
    "    '''\n",
    "    Converts arguments in word sequence saving it in global sequences array \n",
    "    Updates the global max sequence length.\n",
    "    \n",
    "    Arguments:\n",
    "        num_df: theme number to process\n",
    "    '''\n",
    "    global MAX_SEQUENCE_LENGTH, df_x, sequences\n",
    "    df_x[num_df] = np.array(df[num_df]['x']['train'] + df[num_df]['x']['dev'] + df[num_df]['x']['test'])\n",
    "    # to list of words\n",
    "    sequences[num_df] = []\n",
    "    for argument_j in range(0, df_x[num_df].shape[0]):\n",
    "        in_unicode = df_x[num_df][argument_j].decode('utf-8')\n",
    "        in_string = unicodedata.normalize('NFKD', in_unicode).encode('ascii','ignore')\n",
    "        if argument_j == 7291:\n",
    "            especial = text_to_word_sequence(in_string)\n",
    "        sequences[num_df].append(text_to_word_sequence(in_string))\n",
    "    # search for the biggest\n",
    "    for sequence in sequences[num_df]:\n",
    "        if len(sequence) > MAX_SEQUENCE_LENGTH:\n",
    "            MAX_SEQUENCE_LENGTH = len(sequence)\n",
    "\n",
    "def get_x_sets(num_df):\n",
    "    '''\n",
    "    Replaces word in sequences for corresponding numbers.\n",
    "    Arguments:\n",
    "        num_df: theme number from which to get the sets\n",
    "    Returns:\n",
    "        Train, development and test set\n",
    "    '''\n",
    "    global df_x, sequences\n",
    "    # every X[i] with max size\n",
    "    # replace words by numbers with world_dict\n",
    "    X = np.zeros((df_x[num_df].shape[0], MAX_SEQUENCE_LENGTH)).astype(int)\n",
    "    for i in range(0, len(sequences[num_df])):\n",
    "        for j in range(0, len(sequences[num_df][i])):\n",
    "            X[i][-len(sequences[num_df][i])+j] = word_to_num[sequences[num_df][i][j]]\n",
    "    # divide sets for answer\n",
    "    TRAIN_SIZE = len(df[num_df]['x']['train'])\n",
    "    DEV_SIZE = len(df[num_df]['x']['dev'])\n",
    "    TEST_SIZE = len(df[num_df]['x']['test'])\n",
    "    X_train = X[0 : TRAIN_SIZE]\n",
    "    X_dev = X[TRAIN_SIZE : TRAIN_SIZE+DEV_SIZE]\n",
    "    X_test = X[TRAIN_SIZE+DEV_SIZE : ]\n",
    "    return X_train, X_dev, X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    preprocess_x(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poder',\n",
       " 'judicial',\n",
       " 'probo',\n",
       " 'ecuanime',\n",
       " 'y',\n",
       " 'equitativo',\n",
       " 'respeto',\n",
       " 'absoluto',\n",
       " 'a',\n",
       " 'la',\n",
       " 'ley']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train best configuration models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dan(relu_layers=3, hidden_units=300, p_dropout=0.3, dropout_input=False, \n",
    "        my_regularizer=regularizers.l2(1e-5), my_optimizer='adam', \n",
    "        epochs=150, batch_size=200):\n",
    "    '''\n",
    "    Creates and fit NN. \n",
    "    NN Arquitecture: \n",
    "        Input: vector with numbers representing index in embedding layer\n",
    "        Embedding layer: matrix multiplication to obtain vectors for each index in the input\n",
    "        Dropout: optional words dropout\n",
    "        Mean: Averages the vectors of the embedding output, returning one averaged vector\n",
    "        Fully connected: Fully connected layers with relu as activation function\n",
    "                         optional neuron dropout\n",
    "        Fully connected: output layer with softmax function\n",
    "    \n",
    "    Arguments:\n",
    "        relu_layers: Number of fully connected layers with relu \n",
    "        hidden_units: Number of neurons on the relu layers\n",
    "        p_dropout: dropout probability for the relu layers\n",
    "        dropout_input: dropout probability for words \n",
    "        my_regularizer: kernel_regularizer for fully connected layers\n",
    "        my_optimizer: optimizer for back propagation\n",
    "        epochs: number of epochs to train\n",
    "        batch_size: batch_size for trainning\n",
    "    '''\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "    x = Embedding(len(word_to_num) + 1, EMBEDDING_DIM, mask_zero=True,\n",
    "                  weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "    if dropout_input:\n",
    "        x = Dropout(0.5)(x)\n",
    "    x = Lambda(lambda x: K.mean(x, axis=1), \n",
    "               output_shape=(embedding_matrix.shape[1],))(x)\n",
    "    for i in range(0, relu_layers):\n",
    "        x = Dropout(p_dropout)(x)\n",
    "        x = Dense(units=hidden_units, activation='relu', kernel_regularizer=my_regularizer)(x)\n",
    "    preds = Dense(units=len(encoder.classes_), activation='softmax', \n",
    "                  kernel_regularizer=my_regularizer)(x)\n",
    "    \n",
    "    m = Model(sequence_input, preds)\n",
    "    m.compile(loss='categorical_crossentropy', optimizer=my_optimizer, \n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "    print \"Starting trainning\"\n",
    "    m.fit(X_train, y_train,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          shuffle=True, \n",
    "          epochs=epochs, batch_size=batch_size,\n",
    "          verbose=1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- TEMA 1--------------------------\n",
      "Starting trainning\n",
      "Train on 36886 samples, validate on 4611 samples\n",
      "Epoch 1/80\n",
      "36886/36886 [==============================] - 18s - loss: 2.8830 - acc: 0.2159 - top_k_categorical_accuracy: 0.5109 - val_loss: 2.3633 - val_acc: 0.3570 - val_top_k_categorical_accuracy: 0.6790\n",
      "Epoch 2/80\n",
      "36886/36886 [==============================] - 18s - loss: 2.2399 - acc: 0.3771 - top_k_categorical_accuracy: 0.7120 - val_loss: 1.9800 - val_acc: 0.4433 - val_top_k_categorical_accuracy: 0.7664\n",
      "Epoch 3/80\n",
      "36886/36886 [==============================] - 20s - loss: 2.0150 - acc: 0.4317 - top_k_categorical_accuracy: 0.7635 - val_loss: 1.8355 - val_acc: 0.4880 - val_top_k_categorical_accuracy: 0.7998\n",
      "Epoch 4/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.8984 - acc: 0.4648 - top_k_categorical_accuracy: 0.7885 - val_loss: 1.7137 - val_acc: 0.5255 - val_top_k_categorical_accuracy: 0.8265\n",
      "Epoch 5/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.8203 - acc: 0.4872 - top_k_categorical_accuracy: 0.8049 - val_loss: 1.6384 - val_acc: 0.5485 - val_top_k_categorical_accuracy: 0.8365\n",
      "Epoch 6/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.7678 - acc: 0.5030 - top_k_categorical_accuracy: 0.8178 - val_loss: 1.6053 - val_acc: 0.5532 - val_top_k_categorical_accuracy: 0.8404\n",
      "Epoch 7/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.7269 - acc: 0.5164 - top_k_categorical_accuracy: 0.8237 - val_loss: 1.5413 - val_acc: 0.5654 - val_top_k_categorical_accuracy: 0.8527\n",
      "Epoch 8/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.6927 - acc: 0.5233 - top_k_categorical_accuracy: 0.8292 - val_loss: 1.5003 - val_acc: 0.5795 - val_top_k_categorical_accuracy: 0.8579\n",
      "Epoch 9/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.6668 - acc: 0.5312 - top_k_categorical_accuracy: 0.8338 - val_loss: 1.4950 - val_acc: 0.5808 - val_top_k_categorical_accuracy: 0.8603\n",
      "Epoch 10/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.6482 - acc: 0.5362 - top_k_categorical_accuracy: 0.8367 - val_loss: 1.4734 - val_acc: 0.5879 - val_top_k_categorical_accuracy: 0.8666\n",
      "Epoch 11/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.6286 - acc: 0.5441 - top_k_categorical_accuracy: 0.8402 - val_loss: 1.4477 - val_acc: 0.5997 - val_top_k_categorical_accuracy: 0.8679\n",
      "Epoch 12/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.6114 - acc: 0.5462 - top_k_categorical_accuracy: 0.8428 - val_loss: 1.4422 - val_acc: 0.5966 - val_top_k_categorical_accuracy: 0.8692\n",
      "Epoch 13/80\n",
      "36886/36886 [==============================] - 22s - loss: 1.5939 - acc: 0.5512 - top_k_categorical_accuracy: 0.8459 - val_loss: 1.4214 - val_acc: 0.6003 - val_top_k_categorical_accuracy: 0.8733\n",
      "Epoch 14/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.5881 - acc: 0.5529 - top_k_categorical_accuracy: 0.8470 - val_loss: 1.4179 - val_acc: 0.6023 - val_top_k_categorical_accuracy: 0.8710\n",
      "Epoch 15/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.5772 - acc: 0.5543 - top_k_categorical_accuracy: 0.8486 - val_loss: 1.3942 - val_acc: 0.6072 - val_top_k_categorical_accuracy: 0.8762\n",
      "Epoch 16/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.5648 - acc: 0.5605 - top_k_categorical_accuracy: 0.8491 - val_loss: 1.3947 - val_acc: 0.6101 - val_top_k_categorical_accuracy: 0.8753\n",
      "Epoch 17/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.5560 - acc: 0.5644 - top_k_categorical_accuracy: 0.8528 - val_loss: 1.3915 - val_acc: 0.6140 - val_top_k_categorical_accuracy: 0.8792\n",
      "Epoch 18/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.5457 - acc: 0.5638 - top_k_categorical_accuracy: 0.8538 - val_loss: 1.3723 - val_acc: 0.6172 - val_top_k_categorical_accuracy: 0.8794\n",
      "Epoch 19/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.5394 - acc: 0.5664 - top_k_categorical_accuracy: 0.8546 - val_loss: 1.3868 - val_acc: 0.6122 - val_top_k_categorical_accuracy: 0.8779\n",
      "Epoch 20/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.5323 - acc: 0.5676 - top_k_categorical_accuracy: 0.8557 - val_loss: 1.3681 - val_acc: 0.6181 - val_top_k_categorical_accuracy: 0.8803\n",
      "Epoch 21/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.5283 - acc: 0.5682 - top_k_categorical_accuracy: 0.8549 - val_loss: 1.3661 - val_acc: 0.6161 - val_top_k_categorical_accuracy: 0.8820\n",
      "Epoch 22/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.5199 - acc: 0.5690 - top_k_categorical_accuracy: 0.8577 - val_loss: 1.3686 - val_acc: 0.6133 - val_top_k_categorical_accuracy: 0.8812\n",
      "Epoch 23/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.5114 - acc: 0.5721 - top_k_categorical_accuracy: 0.8596 - val_loss: 1.3534 - val_acc: 0.6226 - val_top_k_categorical_accuracy: 0.8788\n",
      "Epoch 24/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.5070 - acc: 0.5740 - top_k_categorical_accuracy: 0.8598 - val_loss: 1.3491 - val_acc: 0.6229 - val_top_k_categorical_accuracy: 0.8831\n",
      "Epoch 25/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.5018 - acc: 0.5743 - top_k_categorical_accuracy: 0.8601 - val_loss: 1.3489 - val_acc: 0.6209 - val_top_k_categorical_accuracy: 0.8820\n",
      "Epoch 26/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4928 - acc: 0.5751 - top_k_categorical_accuracy: 0.8648 - val_loss: 1.3326 - val_acc: 0.6261 - val_top_k_categorical_accuracy: 0.8838\n",
      "Epoch 27/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.4923 - acc: 0.5775 - top_k_categorical_accuracy: 0.8614 - val_loss: 1.3391 - val_acc: 0.6250 - val_top_k_categorical_accuracy: 0.8833\n",
      "Epoch 28/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.4916 - acc: 0.5760 - top_k_categorical_accuracy: 0.8621 - val_loss: 1.3328 - val_acc: 0.6220 - val_top_k_categorical_accuracy: 0.8857\n",
      "Epoch 29/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.4837 - acc: 0.5794 - top_k_categorical_accuracy: 0.8628 - val_loss: 1.3363 - val_acc: 0.6192 - val_top_k_categorical_accuracy: 0.8786\n",
      "Epoch 30/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.4733 - acc: 0.5807 - top_k_categorical_accuracy: 0.8642 - val_loss: 1.3334 - val_acc: 0.6291 - val_top_k_categorical_accuracy: 0.8833\n",
      "Epoch 31/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.4731 - acc: 0.5826 - top_k_categorical_accuracy: 0.8658 - val_loss: 1.3179 - val_acc: 0.6294 - val_top_k_categorical_accuracy: 0.8870\n",
      "Epoch 32/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4703 - acc: 0.5828 - top_k_categorical_accuracy: 0.8645 - val_loss: 1.3129 - val_acc: 0.6322 - val_top_k_categorical_accuracy: 0.8868\n",
      "Epoch 33/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4639 - acc: 0.5848 - top_k_categorical_accuracy: 0.8668 - val_loss: 1.3213 - val_acc: 0.6324 - val_top_k_categorical_accuracy: 0.8838\n",
      "Epoch 34/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4597 - acc: 0.5849 - top_k_categorical_accuracy: 0.8682 - val_loss: 1.3094 - val_acc: 0.6302 - val_top_k_categorical_accuracy: 0.8870\n",
      "Epoch 35/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4638 - acc: 0.5833 - top_k_categorical_accuracy: 0.8671 - val_loss: 1.3233 - val_acc: 0.6324 - val_top_k_categorical_accuracy: 0.8838\n",
      "Epoch 36/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4580 - acc: 0.5854 - top_k_categorical_accuracy: 0.8674 - val_loss: 1.3113 - val_acc: 0.6326 - val_top_k_categorical_accuracy: 0.8851\n",
      "Epoch 37/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4583 - acc: 0.5858 - top_k_categorical_accuracy: 0.8670 - val_loss: 1.3115 - val_acc: 0.6287 - val_top_k_categorical_accuracy: 0.8883\n",
      "Epoch 38/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4449 - acc: 0.5897 - top_k_categorical_accuracy: 0.8690 - val_loss: 1.3090 - val_acc: 0.6393 - val_top_k_categorical_accuracy: 0.8890\n",
      "Epoch 39/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4455 - acc: 0.5875 - top_k_categorical_accuracy: 0.8708 - val_loss: 1.3110 - val_acc: 0.6311 - val_top_k_categorical_accuracy: 0.8861\n",
      "Epoch 40/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36886/36886 [==============================] - 18s - loss: 1.4451 - acc: 0.5891 - top_k_categorical_accuracy: 0.8686 - val_loss: 1.3145 - val_acc: 0.6246 - val_top_k_categorical_accuracy: 0.8868\n",
      "Epoch 41/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4384 - acc: 0.5912 - top_k_categorical_accuracy: 0.8716 - val_loss: 1.2960 - val_acc: 0.6370 - val_top_k_categorical_accuracy: 0.8892\n",
      "Epoch 42/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4396 - acc: 0.5900 - top_k_categorical_accuracy: 0.8709 - val_loss: 1.3094 - val_acc: 0.6328 - val_top_k_categorical_accuracy: 0.8898\n",
      "Epoch 43/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4325 - acc: 0.5904 - top_k_categorical_accuracy: 0.8712 - val_loss: 1.3012 - val_acc: 0.6359 - val_top_k_categorical_accuracy: 0.8894\n",
      "Epoch 44/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4239 - acc: 0.5947 - top_k_categorical_accuracy: 0.8720 - val_loss: 1.2966 - val_acc: 0.6352 - val_top_k_categorical_accuracy: 0.8894\n",
      "Epoch 45/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4231 - acc: 0.5942 - top_k_categorical_accuracy: 0.8709 - val_loss: 1.3100 - val_acc: 0.6287 - val_top_k_categorical_accuracy: 0.8881\n",
      "Epoch 46/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4260 - acc: 0.5929 - top_k_categorical_accuracy: 0.8721 - val_loss: 1.3060 - val_acc: 0.6318 - val_top_k_categorical_accuracy: 0.8868\n",
      "Epoch 47/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4253 - acc: 0.5915 - top_k_categorical_accuracy: 0.8716 - val_loss: 1.2910 - val_acc: 0.6361 - val_top_k_categorical_accuracy: 0.8887\n",
      "Epoch 48/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4160 - acc: 0.5942 - top_k_categorical_accuracy: 0.8733 - val_loss: 1.3071 - val_acc: 0.6352 - val_top_k_categorical_accuracy: 0.8877\n",
      "Epoch 49/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4127 - acc: 0.5969 - top_k_categorical_accuracy: 0.8750 - val_loss: 1.2835 - val_acc: 0.6413 - val_top_k_categorical_accuracy: 0.8920\n",
      "Epoch 50/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4147 - acc: 0.5961 - top_k_categorical_accuracy: 0.8743 - val_loss: 1.2901 - val_acc: 0.6337 - val_top_k_categorical_accuracy: 0.8898\n",
      "Epoch 51/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4120 - acc: 0.5928 - top_k_categorical_accuracy: 0.8757 - val_loss: 1.2849 - val_acc: 0.6400 - val_top_k_categorical_accuracy: 0.8922\n",
      "Epoch 52/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4148 - acc: 0.5941 - top_k_categorical_accuracy: 0.8749 - val_loss: 1.2939 - val_acc: 0.6346 - val_top_k_categorical_accuracy: 0.8913\n",
      "Epoch 53/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.4038 - acc: 0.5960 - top_k_categorical_accuracy: 0.8773 - val_loss: 1.2916 - val_acc: 0.6367 - val_top_k_categorical_accuracy: 0.8909\n",
      "Epoch 54/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.4033 - acc: 0.5970 - top_k_categorical_accuracy: 0.8746 - val_loss: 1.2714 - val_acc: 0.6400 - val_top_k_categorical_accuracy: 0.8874\n",
      "Epoch 55/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3982 - acc: 0.6006 - top_k_categorical_accuracy: 0.8774 - val_loss: 1.2960 - val_acc: 0.6354 - val_top_k_categorical_accuracy: 0.8853\n",
      "Epoch 56/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3975 - acc: 0.5988 - top_k_categorical_accuracy: 0.8774 - val_loss: 1.2845 - val_acc: 0.6385 - val_top_k_categorical_accuracy: 0.8907\n",
      "Epoch 57/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3945 - acc: 0.6004 - top_k_categorical_accuracy: 0.8779 - val_loss: 1.2850 - val_acc: 0.6357 - val_top_k_categorical_accuracy: 0.8903\n",
      "Epoch 58/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3988 - acc: 0.5965 - top_k_categorical_accuracy: 0.8756 - val_loss: 1.2744 - val_acc: 0.6413 - val_top_k_categorical_accuracy: 0.8948\n",
      "Epoch 59/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.3937 - acc: 0.6012 - top_k_categorical_accuracy: 0.8779 - val_loss: 1.2825 - val_acc: 0.6393 - val_top_k_categorical_accuracy: 0.8896\n",
      "Epoch 60/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3937 - acc: 0.6007 - top_k_categorical_accuracy: 0.8782 - val_loss: 1.2887 - val_acc: 0.6339 - val_top_k_categorical_accuracy: 0.8868\n",
      "Epoch 61/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3860 - acc: 0.6004 - top_k_categorical_accuracy: 0.8779 - val_loss: 1.2802 - val_acc: 0.6383 - val_top_k_categorical_accuracy: 0.8922\n",
      "Epoch 62/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.3931 - acc: 0.5989 - top_k_categorical_accuracy: 0.8776 - val_loss: 1.2977 - val_acc: 0.6331 - val_top_k_categorical_accuracy: 0.8864\n",
      "Epoch 63/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3812 - acc: 0.6012 - top_k_categorical_accuracy: 0.8779 - val_loss: 1.2809 - val_acc: 0.6376 - val_top_k_categorical_accuracy: 0.8877\n",
      "Epoch 64/80\n",
      "36886/36886 [==============================] - 21s - loss: 1.3840 - acc: 0.6011 - top_k_categorical_accuracy: 0.8793 - val_loss: 1.2842 - val_acc: 0.6385 - val_top_k_categorical_accuracy: 0.8913\n",
      "Epoch 65/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.3829 - acc: 0.6009 - top_k_categorical_accuracy: 0.8788 - val_loss: 1.2886 - val_acc: 0.6400 - val_top_k_categorical_accuracy: 0.8872\n",
      "Epoch 66/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3839 - acc: 0.6026 - top_k_categorical_accuracy: 0.8788 - val_loss: 1.2852 - val_acc: 0.6419 - val_top_k_categorical_accuracy: 0.8885\n",
      "Epoch 67/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3787 - acc: 0.6038 - top_k_categorical_accuracy: 0.8806 - val_loss: 1.2710 - val_acc: 0.6428 - val_top_k_categorical_accuracy: 0.8907\n",
      "Epoch 68/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3773 - acc: 0.6046 - top_k_categorical_accuracy: 0.8795 - val_loss: 1.2767 - val_acc: 0.6439 - val_top_k_categorical_accuracy: 0.8907\n",
      "Epoch 69/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3715 - acc: 0.6043 - top_k_categorical_accuracy: 0.8811 - val_loss: 1.2780 - val_acc: 0.6422 - val_top_k_categorical_accuracy: 0.8905\n",
      "Epoch 70/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3756 - acc: 0.6038 - top_k_categorical_accuracy: 0.8794 - val_loss: 1.2749 - val_acc: 0.6365 - val_top_k_categorical_accuracy: 0.8913\n",
      "Epoch 71/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3689 - acc: 0.6061 - top_k_categorical_accuracy: 0.8817 - val_loss: 1.2769 - val_acc: 0.6402 - val_top_k_categorical_accuracy: 0.8911\n",
      "Epoch 72/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.3712 - acc: 0.6060 - top_k_categorical_accuracy: 0.8803 - val_loss: 1.2785 - val_acc: 0.6352 - val_top_k_categorical_accuracy: 0.8944\n",
      "Epoch 73/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3661 - acc: 0.6076 - top_k_categorical_accuracy: 0.8824 - val_loss: 1.2770 - val_acc: 0.6422 - val_top_k_categorical_accuracy: 0.8905\n",
      "Epoch 74/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3659 - acc: 0.6088 - top_k_categorical_accuracy: 0.8798 - val_loss: 1.2761 - val_acc: 0.6439 - val_top_k_categorical_accuracy: 0.8918\n",
      "Epoch 75/80\n",
      "36886/36886 [==============================] - 20s - loss: 1.3668 - acc: 0.6061 - top_k_categorical_accuracy: 0.8824 - val_loss: 1.2902 - val_acc: 0.6309 - val_top_k_categorical_accuracy: 0.8929\n",
      "Epoch 76/80\n",
      "36886/36886 [==============================] - 19s - loss: 1.3666 - acc: 0.6055 - top_k_categorical_accuracy: 0.8820 - val_loss: 1.2753 - val_acc: 0.6406 - val_top_k_categorical_accuracy: 0.8896\n",
      "Epoch 77/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.3583 - acc: 0.6104 - top_k_categorical_accuracy: 0.8831 - val_loss: 1.2703 - val_acc: 0.6400 - val_top_k_categorical_accuracy: 0.8916\n",
      "Epoch 78/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.3612 - acc: 0.6071 - top_k_categorical_accuracy: 0.8819 - val_loss: 1.2852 - val_acc: 0.6315 - val_top_k_categorical_accuracy: 0.8900\n",
      "Epoch 79/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.3641 - acc: 0.6072 - top_k_categorical_accuracy: 0.8824 - val_loss: 1.2793 - val_acc: 0.6367 - val_top_k_categorical_accuracy: 0.8933\n",
      "Epoch 80/80\n",
      "36886/36886 [==============================] - 18s - loss: 1.3541 - acc: 0.6101 - top_k_categorical_accuracy: 0.8843 - val_loss: 1.2764 - val_acc: 0.6370 - val_top_k_categorical_accuracy: 0.8907\n",
      "36832/36886 [============================>.] - ETA: 0s\n",
      "TRAIN:\n",
      "- loss 1.09570205634\n",
      "- acc 0.681938947013\n",
      "- top_k_categorical_accuracy 0.921460716793\n",
      "4576/4611 [============================>.] - ETA: 0s\n",
      "DEV:\n",
      "- loss 1.27641256518\n",
      "- acc 0.636955107365\n",
      "- top_k_categorical_accuracy 0.890696161366\n",
      "4576/4611 [============================>.] - ETA: 0s\n",
      "TEST:\n",
      "- loss 1.27913025769\n",
      "- acc 0.634786380401\n",
      "- top_k_categorical_accuracy 0.895033615281 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4):\n",
    "    print \"----------------------- TEMA \"+str(i+1)+\"--------------------------\"\n",
    "    y_train, y_dev, y_test = get_y_sets(i)\n",
    "    X_train, X_dev, X_test = get_x_sets(i)\n",
    "    batch_size = 30\n",
    "    epochs = 80\n",
    "    l2 = None if i != 1 else regularizers.l2(1e-5) # the best has not regularizator in theme 2\n",
    "    m = dan(relu_layers=2, hidden_units=200, \n",
    "            p_dropout=0.2, my_regularizer=l2,\n",
    "            epochs=epochs, batch_size=batch_size)\n",
    "    m.save('models/tema'+str(i+1)+'.h5')\n",
    "    score = m.evaluate(X_train, y_train)\n",
    "    print \"\\nTRAIN:\"\n",
    "    print \"-\", m.metrics_names[0], score[0]\n",
    "    print \"-\", m.metrics_names[1], score[1]\n",
    "    print \"-\", m.metrics_names[2], score[2]\n",
    "    score = m.evaluate(X_dev, y_dev)\n",
    "    print \"\\nDEV:\"\n",
    "    print \"-\", m.metrics_names[0], score[0]\n",
    "    print \"-\", m.metrics_names[1], score[1]\n",
    "    print \"-\", m.metrics_names[2], score[2]\n",
    "    score = m.evaluate(X_test, y_test)\n",
    "    print \"\\nTEST:\"\n",
    "    print \"-\", m.metrics_names[0], score[0]\n",
    "    print \"-\", m.metrics_names[1], score[1]\n",
    "    print \"-\", m.metrics_names[2], score[2], '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
