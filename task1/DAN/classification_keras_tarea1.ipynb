{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1\n",
    "Se cargan los datos correspondientes a la tarea 1, la matriz de pesos extraídos de fasttext (embedding layer) y el diccionario de palabras: int. Las id de las palabras en ese diccionario representan el índice en donde se encuentra el vector de esa palabra en el embedding layer.\n",
    "\n",
    "Los identificadores de labels (conceptos) son convertidos a one hot vector\n",
    "\n",
    "La función preprocess_x tiene como objetivo encontrar la secuencia de palabras (argumento) más largo, de tal forma de luego hacer padding a las entradas más pequeñas. Para no tener errores probando otras entradas más largas, se aumentó este valor en 100 para tener rango de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from itertools import product\n",
    "import unicodedata\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Dropout, Input, Embedding, Lambda\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for tema in range(0, 4):\n",
    "    dict_column = {}\n",
    "    for column in ['x', 'y']:\n",
    "        dict_set = {}\n",
    "        for set_ in ['train', 'dev', 'test']:\n",
    "            filename = '../../data/'+column+'_'+set_+'_tema_'+str(tema+1)+'_categorias_pnud_0.txt'\n",
    "            with open(filename) as f:\n",
    "                data = f.readlines()\n",
    "            dict_set[set_] = [row[:-1] for row in data]\n",
    "        dict_column[column] = dict_set\n",
    "    df.append(dict_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x', 'y'])\n",
      "dict_keys(['train', 'dev', 'test'])\n"
     ]
    }
   ],
   "source": [
    "print(df[0].keys())\n",
    "print(df[0]['x'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors and Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "model = FastText.load_word2vec_format('../../word_vectors/ca-vectors.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "vocab_size = len(model.vocab)\n",
    "embedding_matrix = np.zeros((vocab_size+1, EMBEDDING_DIM))\n",
    "for word in model.vocab:\n",
    "    index = model.vocab[word].index\n",
    "    embedding_matrix[index+1, :] = model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global encoder       # to detect number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_y_sets(NUM_DF):\n",
    "    TRAIN_SIZE = len(df[NUM_DF]['x']['train'])\n",
    "    DEV_SIZE = len(df[NUM_DF]['x']['dev'])\n",
    "    TEST_SIZE = len(df[NUM_DF]['x']['test'])\n",
    "    df_y = np.array(df[NUM_DF]['y']['train'] + df[NUM_DF]['y']['dev'] + df[NUM_DF]['y']['test'])\n",
    "    # one hot vector label for clasification\n",
    "    global encoder\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_y) # to know how many classes \n",
    "    labels = encoder.transform(df_y)\n",
    "    Y = to_categorical(np.asarray(labels))\n",
    "    \n",
    "    y_train = Y[0 : TRAIN_SIZE]\n",
    "    y_dev = Y[TRAIN_SIZE : TRAIN_SIZE+DEV_SIZE]\n",
    "    y_test = Y[TRAIN_SIZE+DEV_SIZE : ]\n",
    "    return y_train, y_dev, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global df_x, sequences, MAX_SEQUENCE_LENGTH\n",
    "df_x = [None, None, None, None]              # argumentos de train+dev+test para cada tema\n",
    "sequences = [None, None, None, None]         # argumentos como listas de palabras para cada tema\n",
    "MAX_SEQUENCE_LENGTH = 0                      # tamaño máximo de vector de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_x(num_df):\n",
    "    '''\n",
    "    Converts arguments in word sequence saving it in global sequences array \n",
    "    Updates the global max sequence length.\n",
    "    \n",
    "    Arguments:\n",
    "        num_df: theme number to process\n",
    "    '''\n",
    "    global MAX_SEQUENCE_LENGTH, df_x, sequences\n",
    "    df_x[num_df] = np.array(df[num_df]['x']['train'] + df[num_df]['x']['dev'] + df[num_df]['x']['test'])\n",
    "    # to list of words\n",
    "    sequences[num_df] = []\n",
    "    for argument_j in range(0, df_x[num_df].shape[0]):\n",
    "        in_string = df_x[num_df][argument_j]\n",
    "        sequences[num_df].append(text_to_word_sequence(in_string))\n",
    "    # search for the biggest\n",
    "    for sequence in sequences[num_df]:\n",
    "        if len(sequence) > MAX_SEQUENCE_LENGTH:\n",
    "            MAX_SEQUENCE_LENGTH = len(sequence)\n",
    "\n",
    "def get_x_sets(num_df):\n",
    "    '''\n",
    "    Replaces word in sequences for corresponding numbers.\n",
    "    Arguments:\n",
    "        num_df: theme number from which to get the sets\n",
    "    Returns:\n",
    "        Train, development and test set\n",
    "    '''\n",
    "    global df_x, sequences\n",
    "    # every X[i] with max size\n",
    "    # replace words by numbers with world_dict\n",
    "    index2word_set = set(model.index2word)\n",
    "    X = np.zeros((df_x[num_df].shape[0], MAX_SEQUENCE_LENGTH)).astype(int)\n",
    "    for i in range(0, len(sequences[num_df])):\n",
    "        for j in range(0, len(sequences[num_df][i])):\n",
    "            word = sequences[num_df][i][j]\n",
    "            if word in index2word_set: \n",
    "                X[i][-len(sequences[num_df][i])+j] = model.vocab[word].index+1\n",
    "            else:\n",
    "                X[i][-len(sequences[num_df][i])+j] = 0\n",
    "    # divide sets for answer\n",
    "    TRAIN_SIZE = len(df[num_df]['x']['train'])\n",
    "    DEV_SIZE = len(df[num_df]['x']['dev'])\n",
    "    TEST_SIZE = len(df[num_df]['x']['test'])\n",
    "    X_train = X[0 : TRAIN_SIZE]\n",
    "    X_dev = X[TRAIN_SIZE : TRAIN_SIZE+DEV_SIZE]\n",
    "    X_test = X[TRAIN_SIZE+DEV_SIZE : ]\n",
    "    return X_train, X_dev, X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    preprocess_x(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poder',\n",
       " 'judicial',\n",
       " 'probo',\n",
       " 'ecuánime',\n",
       " 'y',\n",
       " 'equitativo',\n",
       " 'respeto',\n",
       " 'absoluto',\n",
       " 'a',\n",
       " 'la',\n",
       " 'ley']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train best configuration models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rec_prec_f1(scores):\n",
    "    # transforms scores to one hot classification\n",
    "    index_max = np.argmax(scores, axis=1)\n",
    "    y_pred = np.zeros(scores.shape)\n",
    "    for i in range(0, len(index_max)):\n",
    "        y_pred[i][index_max[i]]=1\n",
    "    rec = round(100*metrics.recall_score(y_test, y_pred, average='weighted'), 1)\n",
    "    prec = round(100*metrics.precision_score(y_test, y_pred, average='weighted'), 1)\n",
    "    f1 = round(100*metrics.f1_score(y_test, y_pred, average='weighted'), 1)\n",
    "    return rec, prec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def dan(relu_layers=3, hidden_units=300, p_dropout=0.3, dropout_input=False, \n",
    "        my_regularizer=regularizers.l2(1e-5), my_optimizer='adam', \n",
    "        epochs=150, batch_size=200, trainable=False, filepath=''):\n",
    "    '''\n",
    "    Creates and fit NN. \n",
    "    NN Arquitecture: \n",
    "        Input: vector with numbers representing index in embedding layer\n",
    "        Embedding layer: matrix multiplication to obtain vectors for each index in the input\n",
    "        Dropout: optional words dropout\n",
    "        Mean: Averages the vectors of the embedding output, returning one averaged vector\n",
    "        Fully connected: Fully connected layers with relu as activation function\n",
    "                         optional neuron dropout\n",
    "        Fully connected: output layer with softmax function\n",
    "    \n",
    "    Arguments:\n",
    "        relu_layers: Number of fully connected layers with relu \n",
    "        hidden_units: Number of neurons on the relu layers\n",
    "        p_dropout: dropout probability for the relu layers\n",
    "        dropout_input: dropout probability for words \n",
    "        my_regularizer: kernel_regularizer for fully connected layers\n",
    "        my_optimizer: optimizer for back propagation\n",
    "        epochs: number of epochs to train\n",
    "        batch_size: batch_size for trainning\n",
    "    '''\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "    x = Embedding(len(model.vocab)+1, EMBEDDING_DIM, mask_zero=True,\n",
    "                  weights=[embedding_matrix], trainable=trainable)(sequence_input)\n",
    "    if dropout_input:\n",
    "        x = Dropout(0.2)(x)\n",
    "    x = Lambda(lambda x: K.mean(x, axis=1), \n",
    "               output_shape=(embedding_matrix.shape[1],))(x)\n",
    "    for i in range(0, relu_layers):\n",
    "        x = Dropout(p_dropout)(x)\n",
    "        x = Dense(units=hidden_units, activation='relu', kernel_regularizer=my_regularizer)(x)\n",
    "    preds = Dense(units=len(encoder.classes_), activation='softmax', \n",
    "                  kernel_regularizer=my_regularizer)(x)\n",
    "    \n",
    "    m = Model(sequence_input, preds)\n",
    "    m.compile(loss='categorical_crossentropy', optimizer=my_optimizer, \n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "    print (\"Starting trainning\")\n",
    "    print(relu_layers, hidden_units, p_dropout, dropout_input, \n",
    "          my_regularizer, my_optimizer, epochs, batch_size, \n",
    "          trainable, filepath)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                                 save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    m.fit(X_train, y_train,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          shuffle=True, \n",
    "          epochs=epochs, batch_size=batch_size,\n",
    "          verbose=0,\n",
    "          callbacks=callbacks_list)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- TEMA 1 --------------\n",
      "Starting trainning\n",
      "2 200 0.2 False None adam 100 30 False models/checkpoints/best_tema_1.h5\n",
      "Epoch 00000: val_acc improved from -inf to 0.37129, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00001: val_acc improved from 0.37129 to 0.44416, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00002: val_acc improved from 0.44416 to 0.50293, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00003: val_acc improved from 0.50293 to 0.53437, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00004: val_acc improved from 0.53437 to 0.54999, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00005: val_acc improved from 0.54999 to 0.56951, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00006: val_acc improved from 0.56951 to 0.57601, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00007: val_acc improved from 0.57601 to 0.58599, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 00010: val_acc improved from 0.58599 to 0.59336, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00011: val_acc improved from 0.59336 to 0.59922, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00012: val_acc improved from 0.59922 to 0.60399, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 00015: val_acc improved from 0.60399 to 0.60811, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00016: val_acc improved from 0.60811 to 0.61765, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00017: val_acc improved from 0.61765 to 0.61917, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 00020: val_acc improved from 0.61917 to 0.62308, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 00023: val_acc improved from 0.62308 to 0.62698, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00030: val_acc improved from 0.62698 to 0.63045, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 00033: val_acc improved from 0.63045 to 0.63283, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00034: val_acc improved from 0.63283 to 0.63370, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00035: val_acc improved from 0.63370 to 0.63500, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 00039: val_acc did not improve\n",
      "Epoch 00040: val_acc improved from 0.63500 to 0.63565, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00041: val_acc did not improve\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 00043: val_acc improved from 0.63565 to 0.63609, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00044: val_acc did not improve\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 00046: val_acc improved from 0.63609 to 0.63891, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00047: val_acc did not improve\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 00051: val_acc did not improve\n",
      "Epoch 00052: val_acc did not improve\n",
      "Epoch 00053: val_acc did not improve\n",
      "Epoch 00054: val_acc did not improve\n",
      "Epoch 00055: val_acc did not improve\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 00058: val_acc improved from 0.63891 to 0.64021, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00059: val_acc improved from 0.64021 to 0.64086, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 00061: val_acc did not improve\n",
      "Epoch 00062: val_acc did not improve\n",
      "Epoch 00063: val_acc improved from 0.64086 to 0.64086, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00064: val_acc improved from 0.64086 to 0.64216, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00065: val_acc did not improve\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 00067: val_acc did not improve\n",
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 00069: val_acc improved from 0.64216 to 0.64259, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 00071: val_acc did not improve\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 00073: val_acc did not improve\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 00076: val_acc did not improve\n",
      "Epoch 00077: val_acc improved from 0.64259 to 0.64693, saving model to models/checkpoints/best_tema_1.h5\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 00081: val_acc did not improve\n",
      "Epoch 00082: val_acc did not improve\n",
      "Epoch 00083: val_acc did not improve\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 00085: val_acc did not improve\n",
      "Epoch 00086: val_acc did not improve\n",
      "Epoch 00087: val_acc did not improve\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 00090: val_acc did not improve\n",
      "Epoch 00091: val_acc did not improve\n",
      "Epoch 00092: val_acc did not improve\n",
      "Epoch 00093: val_acc did not improve\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 00095: val_acc did not improve\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 00097: val_acc did not improve\n",
      "Epoch 00098: val_acc did not improve\n",
      "Epoch 00099: val_acc did not improve\n",
      "4480/4611 [============================>.] - ETA: 0sacc: 64.43\n",
      "top_k_categorical_accuracy: 89.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/constanzafierro/tensorflow-p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/constanzafierro/tensorflow-p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  64.4 \n",
      "Precision:  65.2 \n",
      "F1:  63.9\n",
      "------------- TEMA 2 --------------\n",
      "Starting trainning\n",
      "2 200 0.2 False <keras.regularizers.L1L2 object at 0x15fdb6fd0> adam 100 30 False models/checkpoints/best_tema_2.h5\n",
      "Epoch 00000: val_acc improved from -inf to 0.35255, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00001: val_acc improved from 0.35255 to 0.47426, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00002: val_acc improved from 0.47426 to 0.53809, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00003: val_acc improved from 0.53809 to 0.56577, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00004: val_acc improved from 0.56577 to 0.59208, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00005: val_acc improved from 0.59208 to 0.59780, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00006: val_acc improved from 0.59780 to 0.61084, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00007: val_acc improved from 0.61084 to 0.61794, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00008: val_acc improved from 0.61794 to 0.62091, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00009: val_acc improved from 0.62091 to 0.63029, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00010: val_acc improved from 0.63029 to 0.63212, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00011: val_acc improved from 0.63212 to 0.64722, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 00013: val_acc improved from 0.64722 to 0.65088, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00014: val_acc improved from 0.65088 to 0.65500, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00015: val_acc improved from 0.65500 to 0.65729, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00016: val_acc improved from 0.65729 to 0.66186, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 00019: val_acc improved from 0.66186 to 0.67239, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 00025: val_acc improved from 0.67239 to 0.67261, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00026: val_acc improved from 0.67261 to 0.67673, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 00028: val_acc improved from 0.67673 to 0.67833, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00030: val_acc improved from 0.67833 to 0.68245, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 00034: val_acc improved from 0.68245 to 0.68337, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 00039: val_acc improved from 0.68337 to 0.68657, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 00041: val_acc improved from 0.68657 to 0.68932, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 00043: val_acc improved from 0.68932 to 0.69229, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00044: val_acc did not improve\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 00046: val_acc did not improve\n",
      "Epoch 00047: val_acc did not improve\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 00051: val_acc did not improve\n",
      "Epoch 00052: val_acc did not improve\n",
      "Epoch 00053: val_acc did not improve\n",
      "Epoch 00054: val_acc improved from 0.69229 to 0.69366, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00055: val_acc improved from 0.69366 to 0.69481, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 00058: val_acc improved from 0.69481 to 0.69687, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00059: val_acc did not improve\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 00061: val_acc did not improve\n",
      "Epoch 00062: val_acc did not improve\n",
      "Epoch 00063: val_acc did not improve\n",
      "Epoch 00064: val_acc did not improve\n",
      "Epoch 00065: val_acc did not improve\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 00067: val_acc improved from 0.69687 to 0.69755, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 00069: val_acc did not improve\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 00071: val_acc did not improve\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 00073: val_acc improved from 0.69755 to 0.69801, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 00076: val_acc did not improve\n",
      "Epoch 00077: val_acc improved from 0.69801 to 0.69938, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 00081: val_acc did not improve\n",
      "Epoch 00082: val_acc did not improve\n",
      "Epoch 00083: val_acc did not improve\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 00085: val_acc did not improve\n",
      "Epoch 00086: val_acc did not improve\n",
      "Epoch 00087: val_acc did not improve\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 00090: val_acc did not improve\n",
      "Epoch 00091: val_acc did not improve\n",
      "Epoch 00092: val_acc improved from 0.69938 to 0.70030, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00093: val_acc did not improve\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 00095: val_acc did not improve\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 00097: val_acc improved from 0.70030 to 0.70259, saving model to models/checkpoints/best_tema_2.h5\n",
      "Epoch 00098: val_acc did not improve\n",
      "Epoch 00099: val_acc did not improve\n",
      "4352/4370 [============================>.] - ETA: 0sacc: 67.78\n",
      "top_k_categorical_accuracy: 91.12\n",
      "Recall:  67.8 \n",
      "Precision:  67.9 \n",
      "F1:  66.7\n",
      "------------- TEMA 3 --------------\n",
      "Starting trainning\n",
      "2 200 0.2 False None adam 100 30 False models/checkpoints/best_tema_3.h5\n",
      "Epoch 00000: val_acc improved from -inf to 0.57189, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00001: val_acc improved from 0.57189 to 0.65911, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00002: val_acc improved from 0.65911 to 0.68228, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00003: val_acc improved from 0.68228 to 0.70272, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00004: val_acc improved from 0.70272 to 0.70745, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 00006: val_acc improved from 0.70745 to 0.71119, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00007: val_acc improved from 0.71119 to 0.71393, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00008: val_acc improved from 0.71393 to 0.71518, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00009: val_acc improved from 0.71518 to 0.72190, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 00012: val_acc improved from 0.72190 to 0.72215, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 00014: val_acc improved from 0.72215 to 0.72489, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 00018: val_acc improved from 0.72489 to 0.72564, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 00021: val_acc improved from 0.72564 to 0.73262, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 00024: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00025: val_acc improved from 0.73262 to 0.73386, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 00028: val_acc improved from 0.73386 to 0.73586, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 00039: val_acc did not improve\n",
      "Epoch 00040: val_acc improved from 0.73586 to 0.73686, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00041: val_acc improved from 0.73686 to 0.73760, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 00043: val_acc improved from 0.73760 to 0.73785, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00044: val_acc did not improve\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 00046: val_acc did not improve\n",
      "Epoch 00047: val_acc improved from 0.73785 to 0.73885, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00048: val_acc improved from 0.73885 to 0.73985, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 00051: val_acc improved from 0.73985 to 0.74034, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00052: val_acc did not improve\n",
      "Epoch 00053: val_acc did not improve\n",
      "Epoch 00054: val_acc did not improve\n",
      "Epoch 00055: val_acc improved from 0.74034 to 0.74333, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 00058: val_acc did not improve\n",
      "Epoch 00059: val_acc did not improve\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 00061: val_acc did not improve\n",
      "Epoch 00062: val_acc did not improve\n",
      "Epoch 00063: val_acc did not improve\n",
      "Epoch 00064: val_acc improved from 0.74333 to 0.74682, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00065: val_acc did not improve\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 00067: val_acc did not improve\n",
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 00069: val_acc did not improve\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 00071: val_acc did not improve\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 00073: val_acc did not improve\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 00076: val_acc did not improve\n",
      "Epoch 00077: val_acc did not improve\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 00081: val_acc did not improve\n",
      "Epoch 00082: val_acc did not improve\n",
      "Epoch 00083: val_acc did not improve\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 00085: val_acc did not improve\n",
      "Epoch 00086: val_acc did not improve\n",
      "Epoch 00087: val_acc did not improve\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 00090: val_acc did not improve\n",
      "Epoch 00091: val_acc did not improve\n",
      "Epoch 00092: val_acc did not improve\n",
      "Epoch 00093: val_acc improved from 0.74682 to 0.74882, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 00095: val_acc improved from 0.74882 to 0.74956, saving model to models/checkpoints/best_tema_3.h5\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 00097: val_acc did not improve\n",
      "Epoch 00098: val_acc did not improve\n",
      "Epoch 00099: val_acc did not improve\n",
      "3872/4013 [===========================>..] - ETA: 0sacc: 74.16\n",
      "top_k_categorical_accuracy: 96.11\n",
      "Recall:  74.2 \n",
      "Precision:  74.2 \n",
      "F1:  73.9\n",
      "------------- TEMA 4 --------------\n",
      "Starting trainning\n",
      "2 200 0.2 False None adam 100 30 False models/checkpoints/best_tema_4.h5\n",
      "Epoch 00000: val_acc improved from -inf to 0.43868, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00001: val_acc improved from 0.43868 to 0.51272, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00002: val_acc improved from 0.51272 to 0.55394, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00003: val_acc improved from 0.55394 to 0.58982, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00004: val_acc improved from 0.58982 to 0.59415, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00005: val_acc improved from 0.59415 to 0.61476, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00006: val_acc improved from 0.61476 to 0.62188, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00007: val_acc improved from 0.62188 to 0.62239, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00008: val_acc improved from 0.62239 to 0.63435, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00009: val_acc improved from 0.63435 to 0.64046, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00010: val_acc improved from 0.64046 to 0.64758, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 00014: val_acc improved from 0.64758 to 0.64987, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 00017: val_acc improved from 0.64987 to 0.65318, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00018: val_acc improved from 0.65318 to 0.65929, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00019: val_acc improved from 0.65929 to 0.66285, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 00024: val_acc improved from 0.66285 to 0.66412, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 00027: val_acc improved from 0.66412 to 0.66438, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00030: val_acc improved from 0.66438 to 0.66972, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 00034: val_acc improved from 0.66972 to 0.67303, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00035: val_acc improved from 0.67303 to 0.67455, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 00039: val_acc did not improve\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 00041: val_acc did not improve\n",
      "Epoch 00042: val_acc improved from 0.67455 to 0.67532, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00043: val_acc improved from 0.67532 to 0.67532, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00044: val_acc improved from 0.67532 to 0.67786, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 00046: val_acc did not improve\n",
      "Epoch 00047: val_acc did not improve\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 00051: val_acc did not improve\n",
      "Epoch 00052: val_acc improved from 0.67786 to 0.67863, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00053: val_acc did not improve\n",
      "Epoch 00054: val_acc improved from 0.67863 to 0.68092, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00055: val_acc did not improve\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 00058: val_acc improved from 0.68092 to 0.68295, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00059: val_acc improved from 0.68295 to 0.68677, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 00061: val_acc did not improve\n",
      "Epoch 00062: val_acc did not improve\n",
      "Epoch 00063: val_acc did not improve\n",
      "Epoch 00064: val_acc did not improve\n",
      "Epoch 00065: val_acc did not improve\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 00067: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 00069: val_acc did not improve\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 00071: val_acc did not improve\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 00073: val_acc did not improve\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 00076: val_acc improved from 0.68677 to 0.68728, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00077: val_acc did not improve\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 00081: val_acc improved from 0.68728 to 0.68753, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00082: val_acc did not improve\n",
      "Epoch 00083: val_acc did not improve\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 00085: val_acc did not improve\n",
      "Epoch 00086: val_acc did not improve\n",
      "Epoch 00087: val_acc improved from 0.68753 to 0.68931, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 00090: val_acc did not improve\n",
      "Epoch 00091: val_acc did not improve\n",
      "Epoch 00092: val_acc did not improve\n",
      "Epoch 00093: val_acc improved from 0.68931 to 0.68931, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 00095: val_acc improved from 0.68931 to 0.69186, saving model to models/checkpoints/best_tema_4.h5\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 00097: val_acc did not improve\n",
      "Epoch 00098: val_acc did not improve\n",
      "Epoch 00099: val_acc did not improve\n",
      "3872/3930 [============================>.] - ETA: 0sacc: 66.46\n",
      "top_k_categorical_accuracy: 91.40\n",
      "Recall:  66.5 \n",
      "Precision:  66.7 \n",
      "F1:  65.8\n"
     ]
    }
   ],
   "source": [
    "relu_layers=2\n",
    "hidden_units=200\n",
    "p_dropout=0.2\n",
    "batch_size = 30\n",
    "epochs = 100\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print(\"------------- TEMA \"+str(i+1)+\" --------------\")\n",
    "    y_train, y_dev, y_test = get_y_sets(i)\n",
    "    X_train, X_dev, X_test = get_x_sets(i)\n",
    "    l2 = None if i != 1 else regularizers.l2(1e-5) # the best has not regularizator in theme 2\n",
    "    m = dan(relu_layers=relu_layers, hidden_units=hidden_units, \n",
    "            p_dropout=p_dropout, my_regularizer=l2,\n",
    "            epochs=epochs, batch_size=batch_size,\n",
    "            filepath='models/checkpoints/best_tema_' + str(i+1) + '.h5')\n",
    "    m = load_model('models/checkpoints/best_tema_' + str(i+1) + '.h5')\n",
    "    results = m.evaluate(X_test, y_test)\n",
    "    print(\"%s: %0.2f\" % (m.metrics_names[1], round(100*results[1], 2))) # accuracy\n",
    "    print(\"%s: %0.2f\" % (m.metrics_names[2], round(100*results[2], 2))) # top-5\n",
    "    scores = m.predict(X_test, batch_size=batch_size)\n",
    "    rec, prec, f1 = get_rec_prec_f1(scores)\n",
    "    print(\"Recall: \", rec , \"\\nPrecision: \", prec, \"\\nF1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- TEMA 1 --------------\n",
      "4480/4611 [============================>.] - ETA: 0sacc: 64.43\n",
      "top_k_categorical_accuracy: 89.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/constanzafierro/tensorflow-p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/constanzafierro/tensorflow-p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  64.4 \n",
      "Precision:  65.2 \n",
      "F1:  63.9\n",
      "------------- TEMA 2 --------------\n",
      "4320/4370 [============================>.] - ETA: 0sacc: 67.78\n",
      "top_k_categorical_accuracy: 91.12\n",
      "Recall:  67.8 \n",
      "Precision:  67.9 \n",
      "F1:  66.7\n",
      "------------- TEMA 3 --------------\n",
      "3968/4013 [============================>.] - ETA: 0sacc: 74.16\n",
      "top_k_categorical_accuracy: 96.11\n",
      "Recall:  74.2 \n",
      "Precision:  74.2 \n",
      "F1:  73.9\n",
      "------------- TEMA 4 --------------\n",
      "3840/3930 [============================>.] - ETA: 0sacc: 66.46\n",
      "top_k_categorical_accuracy: 91.40\n",
      "Recall:  66.5 \n",
      "Precision:  66.7 \n",
      "F1:  65.8\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4):\n",
    "    print(\"------------- TEMA \"+str(i+1)+\" --------------\")\n",
    "    y_train, y_dev, y_test = get_y_sets(i)\n",
    "    X_train, X_dev, X_test = get_x_sets(i)\n",
    "    \n",
    "    m = load_model('models/checkpoints/best_tema_' + str(i+1) + '.h5')\n",
    "    results = m.evaluate(X_test, y_test)\n",
    "    print(\"%s: %0.2f\" % (m.metrics_names[1], round(100*results[1], 2))) # accuracy\n",
    "    print(\"%s: %0.2f\" % (m.metrics_names[2], round(100*results[2], 2))) # top-5\n",
    "    scores = m.predict(X_test, batch_size=batch_size)\n",
    "    rec, prec, f1 = get_rec_prec_f1(scores)\n",
    "    print(\"Recall: \", rec , \"\\nPrecision: \", prec, \"\\nF1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow python 3",
   "language": "python",
   "name": "tensorflow-p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
