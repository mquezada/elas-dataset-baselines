{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo: \n",
    "\n",
    "- Obtener conversión única de los argumentos desde palabras a números \n",
    "- Crear matriz (embedding layer) con los vectores de fasttext en las posiciones respectivas a la conversión anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from itertools import product\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Dropout, Input, Embedding, Lambda\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "model = FastText.load_fasttext_format('wiki.es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = {'tarea1':[], 'tarea2':[]}\n",
    "for num in ['1','2','3','4']:\n",
    "    parts = {}\n",
    "    for set_data in ['train', 'test', 'dev']:\n",
    "        filename = '../../data/x_'+set_data+'_tema_'+num+'_categorias_pnud_0.txt'\n",
    "        with open(filename) as f:\n",
    "            data = f.readlines()\n",
    "        parts[set_data] = [row[:-1] for row in data]\n",
    "    df['tarea1'].append(parts)\n",
    "    \n",
    "for num in ['1','2','3','4']:\n",
    "    parts = {}\n",
    "    for set_data in ['train', 'test']:\n",
    "        filename = '../../data/tarea2/x_'+set_data+'_tema_'+num+'_categorias_pnud_'\n",
    "        filename += '0.txt' if set_data=='train' else '1.txt'\n",
    "        with open(filename) as f:\n",
    "            data = f.readlines()\n",
    "        parts[set_data] = [row[:-1] for row in data]\n",
    "    df['tarea2'].append(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los gobiernos deben ser trasparentes informando antes de gastar los dineros, informar publicitar los proyectos que se ejecutaran\n",
      "La base de la democracia para que esta sea activa.\n",
      "esencial en cualquier régimen democrático\n"
     ]
    }
   ],
   "source": [
    "print df['tarea1'][0]['train'][0]\n",
    "print df['tarea1'][0]['dev'][0]\n",
    "print df['tarea1'][1]['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_x = []\n",
    "indices = {}\n",
    "for tarea in ['tarea1', 'tarea2']: \n",
    "    for tema in range(0, 4):\n",
    "        for set_ in ['train', 'dev', 'test']:\n",
    "            if tarea == 'tarea2' and set_ == 'dev':\n",
    "                continue\n",
    "            indices[tarea+str(tema)+set_] = len(df_x)\n",
    "            df_x += df[tarea][tema][set_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347919\n",
      "La base de la democracia para que esta sea activa.\n"
     ]
    }
   ],
   "source": [
    "print len(df_x)\n",
    "print df_x[indices['tarea10dev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de la democracia para que esta sea activa.\n"
     ]
    }
   ],
   "source": [
    "# arguments (X) from unicode to str\n",
    "# ignore puntuaction, ñ, accents\n",
    "arguments = []\n",
    "for argument in df_x:\n",
    "    in_unicode = argument.decode('utf-8')\n",
    "    in_string = unicodedata.normalize('NFKD', in_unicode).encode('ascii','ignore')\n",
    "    arguments.append(in_string)\n",
    "print arguments[indices['tarea10dev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 316, 39, 23, 11058, 8498, 941, 1, 6421, 7, 1909, 1529, 8545, 7, 578, 4, 11, 18731]\n"
     ]
    }
   ],
   "source": [
    "# words to numbers (start with 1)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(arguments) # ignore blank spaces, commas, points etc.\n",
    "X = tokenizer.texts_to_sequences(arguments)\n",
    "print X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tokenizer.word_index, open(\"dan_preprocessing_data/dict_word_to_num.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding matrix\n",
    "# matrix with fasttext vectors, where vector in index i is the corresponding of word i in the tokenizer\n",
    "index2word_set = set(model.wv.index2word)\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in index2word_set: \n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_vector = model[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save weights matrix\n",
    "np.savetxt('dan_preprocessing_data/embedding_matrix.txt', embedding_matrix, fmt='%f')\n",
    "# b = np.loadtxt('my_data/embedding_matrix.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38224\n",
      "(38225, 300)\n"
     ]
    }
   ],
   "source": [
    "print len(word_index)\n",
    "print embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
