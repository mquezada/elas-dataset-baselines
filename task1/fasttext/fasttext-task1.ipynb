{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1: clasificar fundamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideramos los modelos precomputados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Carga los modelos creados previamente (OJO, cargar los modelos de vectores preentrendos puede tardar mucho y usar mucha memoria porque pesa 1GB cada uno)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we assume models are already created\n",
    "# these are the global variables used when computing all the metrics\n",
    "# best models in with no pretrained vectors in _model_<tema>_best_at_<k>.bin\n",
    "# best models in with pretrained vectors in _model_<tema>_best_at_<k>.bin\n",
    "\n",
    "import fasttext\n",
    "\n",
    "_dataDir = \"../../data/\"\n",
    "_models_task1 = \"../../task1/fasttext/\"\n",
    "ftlabel = \"__label__\"\n",
    "temas = [1,2,3,4]\n",
    "\n",
    "# prefix and suffixes for the models to load\n",
    "#model_no_ptvec = \"_model_\"\n",
    "#model_ptvec = \"_model_ptvec_\"\n",
    "#at_1 = \"at_1\"\n",
    "#at_5 = \"at_5\"\n",
    "\n",
    "# what models to consider\n",
    "# m_prefixes = [model_no_ptvec]\n",
    "# descomentar lo siguient si se quieren cargar los modelos con vectores preentrenados\n",
    "m_prefixes = [\"_model_\", \"_model_ptvec_\",\"_model_ng2_\",\"_model_ptvec_ng2_\"]\n",
    "m_suffixes = [\"at_1\",\"at_5\"]\n",
    "\n",
    "# names of all considered models, to iterate over models\n",
    "model_names = []\n",
    "for prefix in m_prefixes:\n",
    "    for suffix in m_suffixes:\n",
    "        model_names.append(prefix + suffix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Cargamos solo test set y dev set</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data to predict\n",
    "\n",
    "import string\n",
    "\n",
    "def read_text_file_for_ft_input(filename):\n",
    "    with open(filename) as f:\n",
    "        out = []\n",
    "        for line in f:\n",
    "            # keep tab to separate original concepts from justifications\n",
    "            strdata = \"\".join([c for c in line[:-1] if c not in string.punctuation or c == '\\t']).lower()\n",
    "            if strdata == '':\n",
    "                strdata = ' '\n",
    "            out.append(strdata)\n",
    "    return out\n",
    "\n",
    "def read_numbers_file_for_ft_input(filename):\n",
    "    with open(filename) as f:\n",
    "        out = []\n",
    "        for line in f:\n",
    "            out.append(int(line))\n",
    "    return out\n",
    "\n",
    "test_x = {}\n",
    "test_y = {}\n",
    "\n",
    "dev_x = {}\n",
    "dev_y = {}\n",
    "\n",
    "for i in temas:\n",
    "    test_x[i] = read_text_file_for_ft_input(\n",
    "        _dataDir + \"x_test_tema_\" + str(i) + \"_categorias_pnud_0.txt\")\n",
    "    test_y[i] = read_numbers_file_for_ft_input(\n",
    "        _dataDir + \"y_test_tema_\" + str(i) + \"_categorias_pnud_0.txt\")\n",
    "    dev_x[i] = read_text_file_for_ft_input(\n",
    "        _dataDir + \"x_dev_tema_\" + str(i) + \"_categorias_pnud_0.txt\")\n",
    "    dev_y[i] = read_numbers_file_for_ft_input(\n",
    "        _dataDir + \"y_dev_tema_\" + str(i) + \"_categorias_pnud_0.txt\")\n",
    "    \n",
    "\n",
    "\n",
    "categories = {}\n",
    "for i in temas:\n",
    "    categories[i] = []\n",
    "    # load categories first\n",
    "    categoriesFile = _dataDir + \"categorias_tema_\" + str(i) + \"_pnud_0.txt\"\n",
    "    with open(categoriesFile) as f:\n",
    "        for line in f:\n",
    "            categories[i].append(line[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Computa las predicciones para cada tema y con cada uno de los modelos</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizes = {}\n",
    "sizes[1] = 37\n",
    "sizes[2] = 44\n",
    "sizes[3] = 12\n",
    "sizes[4] = 21\n",
    "\n",
    "predictions_dev_with_labels = {}\n",
    "predictions_with_labels = {}\n",
    "\n",
    "for prefix in m_prefixes:\n",
    "    for suffix in m_suffixes:\n",
    "        model_name = prefix + suffix\n",
    "        predictions_with_labels[model_name] = {}\n",
    "        predictions_dev_with_labels[model_name] = {}\n",
    "        for tema in temas:\n",
    "            model = fasttext.load_model(_models_task1 + prefix + str(tema) + \"_best_\" + suffix + \".bin\")\n",
    "            predictions_dev_with_labels[model_name][tema] = model.predict(\n",
    "                dev_x[tema],k = sizes[tema])\n",
    "            predictions_with_labels[model_name][tema] = model.predict(\n",
    "                test_x[tema],k = sizes[tema])\n",
    "            # free model\n",
    "            # models[prefix + suffix][tema] = None\n",
    "\n",
    "\n",
    "#for model_name in model_names:\n",
    "#    predictions_with_labels[model_name] = {}\n",
    "#    predictions_dev_with_labels[model_name] = {}\n",
    "#    for tema in temas:\n",
    "#        predictions_dev_with_labels[model_name][tema] = models[model_name][tema].predict(\n",
    "#            dev_x[tema],k = sizes[tema])\n",
    "#        predictions_with_labels[model_name][tema] = models[model_name][tema].predict(\n",
    "#            test_x[tema],k = sizes[tema])\n",
    "\n",
    "predictions = {}\n",
    "predictions_dev = {}\n",
    "\n",
    "# cambia las predicciones del tipo <ftlabel><C> a simplemente <C>\n",
    "for model_name in model_names:\n",
    "    predictions_dev[model_name] = {}\n",
    "    predictions[model_name] = {}\n",
    "    for tema in temas:\n",
    "        predictions[model_name][tema] = []\n",
    "        for pred_list_labels in predictions_with_labels[model_name][tema]:\n",
    "            pred_list = []\n",
    "            for label in pred_list_labels:\n",
    "                pred_list.append(int(label[len(ftlabel):]))\n",
    "            predictions[model_name][tema].append(pred_list)\n",
    "        predictions_dev[model_name][tema] = []\n",
    "        for pred_list_labels in predictions_dev_with_labels[model_name][tema]:\n",
    "            pred_list = []\n",
    "            for label in pred_list_labels:\n",
    "                pred_list.append(int(label[len(ftlabel):]))\n",
    "            predictions_dev[model_name][tema].append(pred_list)\n",
    "\n",
    "            \n",
    "# utility function to select the first prediction from a list of predictions and generate a 1D list of single predictions\n",
    "def first_prediction(lists_of_predictions):\n",
    "    out = []\n",
    "    for predictions in lists_of_predictions:\n",
    "        out.append(predictions[0])\n",
    "    return out      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera reportes para primera predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tema 1\t\trecavg\tprecavg\tf1avg\tf1macro\n",
      "devs-nptv\t65.19\t64.08\t64.12\t49.66\n",
      "test-nptv\t65.89\t65.06\t64.9\t51.56\n",
      "devs-ptv\t67.34\t66.26\t66.39\t52.12\n",
      "test-ptv\t67.1\t66.24\t66.24\t53.46\n",
      "devs-npn2\t64.8\t62.44\t63.22\t44.79\n",
      "test-npn2\t64.89\t62.41\t63.29\t44.39\n",
      "devs-ptn2\t68.27\t67.18\t67.28\t51.86\n",
      "test-ptn2\t68.03\t67.12\t67.04\t51.88\n",
      "\n",
      "Tema 2\t\trecavg\tprecavg\tf1avg\tf1macro\n",
      "devs-nptv\t71.84\t71.05\t71.04\t54.68\n",
      "test-nptv\t68.56\t67.95\t67.89\t54.08\n",
      "devs-ptv\t72.27\t71.72\t71.66\t58.49\n",
      "test-ptv\t70.78\t70.49\t70.32\t59.95\n",
      "devs-npn2\t69.25\t67.28\t67.23\t44.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jperez/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/jperez/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-npn2\t67.09\t64.95\t65.05\t42.39\n",
      "devs-ptn2\t72.36\t71.69\t71.67\t54.9\n",
      "test-ptn2\t71.08\t70.45\t70.34\t55.2\n",
      "\n",
      "Tema 3\t\trecavg\tprecavg\tf1avg\tf1macro\n",
      "devs-nptv\t75.85\t75.97\t75.6\t70.28\n",
      "test-nptv\t75.08\t75.14\t74.86\t70.06\n",
      "devs-ptv\t76.38\t76.43\t76.23\t71.87\n",
      "test-ptv\t75.73\t75.72\t75.63\t72.14\n",
      "devs-npn2\t76.85\t76.89\t76.53\t70.41\n",
      "test-npn2\t75.85\t75.86\t75.55\t69.98\n",
      "devs-ptn2\t77.52\t77.54\t77.34\t72.7\n",
      "test-ptn2\t76.9\t76.74\t76.63\t71.13\n",
      "\n",
      "Tema 4\t\trecavg\tprecavg\tf1avg\tf1macro\n",
      "devs-nptv\t69.97\t68.96\t69.06\t60.21\n",
      "test-nptv\t68.45\t67.91\t67.84\t59.0\n",
      "devs-ptv\t70.84\t69.95\t70.06\t61.61\n",
      "test-ptv\t69.34\t68.89\t68.83\t61.23\n",
      "devs-npn2\t69.16\t67.82\t68.04\t57.6\n",
      "test-npn2\t68.55\t67.94\t67.7\t57.82\n",
      "devs-ptn2\t71.35\t70.61\t70.74\t62.46\n",
      "test-ptn2\t69.44\t68.84\t68.89\t59.89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "reported_models = [\"_model_at_1\", \"_model_ptvec_at_1\",\"_model_ng2_at_1\",\"_model_ptvec_ng2_at_1\"]\n",
    "reported_models_labels = ['nptv','ptv','npn2','ptn2']\n",
    "\n",
    "for i in temas:\n",
    "    print(\"Tema \" + str(i) + \"\\t\\trecavg\\tprecavg\\tf1avg\\tf1macro\")\n",
    "    for model,model_name in zip(reported_models,reported_models_labels):\n",
    "        for pred,gold,set_name in zip(\n",
    "            [predictions_dev[model][i],predictions[model][i]],\n",
    "            [dev_y[i],test_y[i]],\n",
    "            ['devs','test']):\n",
    "            prediction = first_prediction(pred)\n",
    "#            acc = round(100*metrics.accuracy_score(gold,prediction),2)\n",
    "            rec = round(100*metrics.recall_score(gold,prediction,average='weighted'),2)\n",
    "            prec = round(100*metrics.precision_score(gold,prediction,average='weighted'),2)\n",
    "            f1 = round(100*metrics.f1_score(gold,prediction,average='weighted'),2)\n",
    "            f1m = round(100*metrics.f1_score(gold,prediction,average='macro'),2)\n",
    "            print(set_name + \"-\" + model_name + \n",
    "#                  \"\\t\"+ str(acc)+ \n",
    "                  \"\\t\"+ str(rec)+\n",
    "                  \"\\t\"+str(prec)+\n",
    "                  \"\\t\"+str(f1)+\n",
    "                  \"\\t\"+str(f1m))\n",
    "#            print(metrics.classification_report(gold,prediction))\n",
    "    print()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera reporte para predicciones en listas (top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to compute top k accuracy\n",
    "\n",
    "def top_k_accuracy(gold,predicted,k):\n",
    "    '''\n",
    "    #Arguments\n",
    "        gold: the true labels of the test cases (size N = number of test cases)\n",
    "        predicted: ranked list of label predictions for every test case (size N x L, where L is assumed to be >= k)\n",
    "        k: the number of elements in the predicted lists that should be considered to compute the metric\n",
    "    #Returns\n",
    "        The portion of cases (between 0 and 1) in which the true label value was among the first k predicted labels\n",
    "    '''\n",
    "    count = 0\n",
    "    for g,pred_labels in zip(gold,predicted):\n",
    "        if g in pred_labels[:k]:\n",
    "                count += 1\n",
    "    return count/len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tema 1\t\t3\t5\t7\t10\n",
      "devs-nptv\t84.23\t89.66\t91.91\t94.47\n",
      "test-nptv\t84.17\t89.44\t92.3\t94.53\n",
      "devs-ptv\t85.34\t90.76\t93.3\t95.45\n",
      "test-ptv\t85.64\t90.72\t93.26\t95.58\n",
      "devs-npn2\t83.0\t88.14\t90.96\t93.67\n",
      "test-npn2\t83.0\t88.22\t90.7\t93.1\n",
      "devs-ptn2\t85.49\t90.26\t92.69\t94.86\n",
      "test-ptn2\t85.14\t90.22\t92.52\t94.64\n",
      "\n",
      "Tema 2\t\t3\t5\t7\t10\n",
      "devs-nptv\t87.69\t91.9\t93.5\t94.99\n",
      "test-nptv\t86.52\t90.62\t93.16\t94.65\n",
      "devs-ptv\t88.86\t92.68\t94.1\t95.7\n",
      "test-ptv\t88.44\t92.33\t94.6\t95.95\n",
      "devs-npn2\t85.98\t90.28\t92.31\t94.12\n",
      "test-npn2\t84.21\t89.08\t91.53\t93.5\n",
      "devs-ptn2\t88.52\t92.24\t93.78\t95.36\n",
      "test-ptn2\t87.85\t91.81\t93.82\t95.63\n",
      "\n",
      "Tema 3\t\t3\t5\t7\t10\n",
      "devs-nptv\t91.63\t95.94\t97.53\t99.08\n",
      "test-nptv\t91.28\t95.81\t97.78\t99.38\n",
      "devs-ptv\t92.25\t96.61\t97.73\t99.1\n",
      "test-ptv\t92.37\t96.36\t98.03\t99.35\n",
      "devs-npn2\t91.58\t95.81\t97.43\t99.2\n",
      "test-npn2\t91.1\t95.37\t97.46\t99.2\n",
      "devs-ptn2\t92.47\t96.51\t97.91\t99.23\n",
      "test-ptn2\t91.78\t95.79\t97.76\t99.38\n",
      "\n",
      "Tema 4\t\t3\t5\t7\t10\n",
      "devs-nptv\t86.49\t91.68\t93.89\t96.21\n",
      "test-nptv\t86.62\t91.15\t93.89\t96.16\n",
      "devs-ptv\t87.35\t92.24\t94.53\t96.69\n",
      "test-ptv\t87.38\t92.52\t94.76\t97.0\n",
      "devs-npn2\t86.41\t91.35\t93.56\t95.67\n",
      "test-npn2\t85.52\t90.97\t93.74\t96.11\n",
      "devs-ptn2\t87.15\t92.44\t94.55\t96.44\n",
      "test-ptn2\t87.4\t92.7\t94.83\t96.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_options = [3,5,7,10]\n",
    "\n",
    "reported_models_at_5 = [\"_model_at_5\", \"_model_ptvec_at_5\",\"_model_ng2_at_5\",\"_model_ptvec_ng2_at_5\"]\n",
    "reported_models_labels_at_5 = ['nptv','ptv','npn2','ptn2']\n",
    "\n",
    "\n",
    "for i in temas:\n",
    "    head_str = \"Tema \" + str(i) + \"\\t\\t\"\n",
    "    head_str += \"\\t\".join([str(op) for op in top_k_options])\n",
    "    print(head_str)\n",
    "    for model, model_name in zip(reported_models_at_5,reported_models_labels_at_5):\n",
    "        for prediction,gold,set_name in zip(\n",
    "                [predictions_dev[model][i],predictions[model][i]],\n",
    "                [dev_y[i],test_y[i]],\n",
    "                ['devs','test']):\n",
    "            data_str = set_name + \"-\" + model_name\n",
    "            for k in top_k_options:\n",
    "                top_k = round(100*top_k_accuracy(gold,prediction,k),2)\n",
    "                data_str += \"\\t\" + str(top_k)\n",
    "            print(data_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ranking_sizes(gold,predicted):\n",
    "    r_sizes = []\n",
    "    for g,pred_labels in zip(gold,predicted):\n",
    "        if g not in pred_labels:\n",
    "            raise Exception('Label ' + str(g) + ' is not in the ranking.')\n",
    "        r_sizes.append(pred_labels.index(g) + 1)\n",
    "    return np.array(r_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tema 1\t\t80%\t85%\t90%\t91%\t92%\t93%\t94%\t95%\n",
      "devs-nptv\t3\t4\t6\t7\t8\t8\t10\t11\n",
      "test-nptv\t3\t4\t6\t7\t7\t8\t9\t11\n",
      "devs-ptv\t2\t3\t5\t6\t6\t7\t8\t10\n",
      "test-ptv\t2\t3\t5\t6\t6\t7\t8\t10\n",
      "devs-npn2\t3\t4\t7\t8\t8\t10\t11\t13\n",
      "test-npn2\t3\t4\t7\t8\t9\t10\t12\t15\n",
      "devs-ptn2\t2\t3\t5\t6\t7\t8\t9\t11\n",
      "test-ptn2\t2\t3\t5\t6\t7\t8\t9\t11\n",
      "\n",
      "Tema 2\t\t80%\t85%\t90%\t91%\t92%\t93%\t94%\t95%\n",
      "devs-nptv\t2\t3\t4\t5\t6\t7\t9\t10\n",
      "test-nptv\t2\t3\t5\t6\t6\t7\t9\t12\n",
      "devs-ptv\t2\t3\t4\t4\t5\t6\t7\t9\n",
      "test-ptv\t2\t3\t4\t5\t5\t6\t7\t8\n",
      "devs-npn2\t2\t3\t5\t6\t7\t8\t10\t12\n",
      "test-npn2\t3\t4\t6\t7\t8\t10\t11\t14\n",
      "devs-ptn2\t2\t3\t4\t4\t5\t6\t8\t10\n",
      "test-ptn2\t2\t3\t4\t5\t6\t7\t8\t9\n",
      "\n",
      "Tema 3\t\t80%\t85%\t90%\t91%\t92%\t93%\t94%\t95%\n",
      "devs-nptv\t2\t2\t3\t3\t4\t4\t4\t5\n",
      "test-nptv\t2\t2\t3\t3\t4\t4\t4\t5\n",
      "devs-ptv\t2\t2\t3\t3\t3\t4\t4\t4\n",
      "test-ptv\t2\t2\t3\t3\t3\t4\t4\t5\n",
      "devs-npn2\t2\t2\t3\t3\t4\t4\t4\t5\n",
      "test-npn2\t2\t2\t3\t3\t4\t4\t5\t5\n",
      "devs-ptn2\t2\t2\t3\t3\t3\t4\t4\t5\n",
      "test-ptn2\t2\t2\t3\t3\t4\t4\t4\t5\n",
      "\n",
      "Tema 4\t\t80%\t85%\t90%\t91%\t92%\t93%\t94%\t95%\n",
      "devs-nptv\t2\t3\t5\t5\t6\t7\t8\t9\n",
      "test-nptv\t2\t3\t5\t5\t6\t7\t8\t9\n",
      "devs-ptv\t2\t3\t5\t5\t5\t6\t7\t8\n",
      "test-ptv\t2\t3\t4\t5\t5\t6\t7\t8\n",
      "devs-npn2\t2\t3\t5\t5\t6\t7\t8\t9\n",
      "test-npn2\t2\t3\t5\t6\t6\t7\t8\t9\n",
      "devs-ptn2\t2\t3\t4\t5\t5\t6\t7\t8\n",
      "test-ptn2\t2\t3\t4\t5\t5\t6\t6\t8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentile_options = [80,85,90,91,92,93,94,95]\n",
    "for i in temas:\n",
    "    head_str = \"Tema \" + str(i) + \"\\t\\t\"\n",
    "    head_str += \"\\t\".join([str(op)+\"%\" for op in percentile_options])\n",
    "    print(head_str)\n",
    "    for model, model_name in zip(reported_models_at_5,reported_models_labels_at_5):\n",
    "        for prediction,gold,set_name in zip(\n",
    "                [predictions_dev[model][i],predictions[model][i]],\n",
    "                [dev_y[i],test_y[i]],\n",
    "                ['devs','test']):\n",
    "            data_str = set_name + \"-\" + model_name\n",
    "            for k in percentile_options:\n",
    "                percentile = int(np.percentile(ranking_sizes(gold,prediction),k))\n",
    "                data_str += \"\\t\" + str(percentile)\n",
    "            print(data_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
