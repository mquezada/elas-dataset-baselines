{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1: clasificar fundamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideramos los modelos precomputados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Carga los modelos creados previamente (OJO cargar los modelos de vectores preentrendos puede tardar mucho y usar mucha memoria porque pesa 1GB cada uno)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we assume models are already created\n",
    "# these are the global variables used when computing all the metrics\n",
    "# best models in with no pretrained vectors in _model_<tema>_best_at_<k>.bin\n",
    "# best models in with pretrained vectors in _model_<tema>_best_at_<k>.bin\n",
    "\n",
    "import fasttext\n",
    "\n",
    "_dataDir = \"../../data/\"\n",
    "_models_task1 = \"../../task1/fasttext/\"\n",
    "ftlabel = \"__label__\"\n",
    "temas = [1,2,3,4]\n",
    "\n",
    "# prefix and suffixes for the models to load\n",
    "model_no_ptvec = \"_model_\"\n",
    "model_ptvec = \"_model_ptvec_\"\n",
    "at_1 = \"at_1\"\n",
    "at_5 = \"at_5\"\n",
    "\n",
    "# what models to consider\n",
    "# m_prefixes = [model_no_ptvec]\n",
    "# descomentar lo siguient si se quieren cargar los modelos con vectores preentrenados\n",
    "m_prefixes = [model_no_ptvec, model_ptvec]\n",
    "m_suffixes = [at_1,at_5]\n",
    "\n",
    "# names of all considered models, to iterate over models\n",
    "model_names = []\n",
    "for prefix in m_prefixes:\n",
    "    for suffix in m_suffixes:\n",
    "        model_names.append(prefix + suffix)\n",
    "        \n",
    "def print_model_name(model):\n",
    "    return model[7:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load models\n",
    "models = {}\n",
    "for prefix in m_prefixes:\n",
    "    for suffix in m_suffixes:\n",
    "        models[prefix + suffix] = {}\n",
    "        for tema in temas:\n",
    "            models[prefix + suffix][tema] =  fasttext.load_model(_models_task1 + prefix + str(tema) + \"_best_\" + suffix + \".bin\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Ahora carga los datos para ajustar los modelos. Cargamos solo test set y dev set</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data to predict\n",
    "\n",
    "import string\n",
    "\n",
    "def read_text_file_for_ft_input(filename):\n",
    "    with open(filename) as f:\n",
    "        out = []\n",
    "        for line in f:\n",
    "            # keep tab to separate original concepts from justifications\n",
    "            strdata = \"\".join([c for c in line[:-1] if c not in string.punctuation or c == '\\t']).lower()\n",
    "            if strdata == '':\n",
    "                strdata = ' '\n",
    "            out.append(strdata)\n",
    "    return out\n",
    "\n",
    "def read_numbers_file_for_ft_input(filename):\n",
    "    with open(filename) as f:\n",
    "        out = []\n",
    "        for line in f:\n",
    "            out.append(int(line))\n",
    "    return out\n",
    "\n",
    "test_x = {}\n",
    "test_y = {}\n",
    "\n",
    "dev_x = {}\n",
    "dev_y = {}\n",
    "\n",
    "for i in temas:\n",
    "    test_x[i] = read_text_file_for_ft_input(\n",
    "        _dataDir + \"x_test_tema_\" + str(i) + \"_categorias_pnud_0.txt\")\n",
    "    test_y[i] = read_numbers_file_for_ft_input(\n",
    "        _dataDir + \"y_test_tema_\" + str(i) + \"_categorias_pnud_0.txt\")\n",
    "    dev_x[i] = read_text_file_for_ft_input(\n",
    "        _dataDir + \"x_dev_tema_\" + str(i) + \"_categorias_pnud_0.txt\")\n",
    "    dev_y[i] = read_numbers_file_for_ft_input(\n",
    "        _dataDir + \"y_dev_tema_\" + str(i) + \"_categorias_pnud_0.txt\")\n",
    "    \n",
    "\n",
    "\n",
    "categories = {}\n",
    "for i in temas:\n",
    "    categories[i] = []\n",
    "    # load categories first\n",
    "    categoriesFile = _dataDir + \"categorias_tema_\" + str(i) + \"_pnud_0.txt\"\n",
    "    with open(categoriesFile) as f:\n",
    "        for line in f:\n",
    "            categories[i].append(line[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Computa las predicciones para cada tema y con cada uno de los modelos</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizes = {}\n",
    "sizes[1] = 37\n",
    "sizes[2] = 44\n",
    "sizes[3] = 12\n",
    "sizes[4] = 21\n",
    "\n",
    "predictions_dev_with_labels = {}\n",
    "predictions_with_labels = {}\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    predictions_with_labels[model_name] = {}\n",
    "    predictions_dev_with_labels[model_name] = {}\n",
    "    for tema in temas:\n",
    "        predictions_dev_with_labels[model_name][tema] = models[model_name][tema].predict(\n",
    "            dev_x[tema],k = sizes[tema])\n",
    "        predictions_with_labels[model_name][tema] = models[model_name][tema].predict(\n",
    "            test_x[tema],k = sizes[tema])\n",
    "\n",
    "predictions = {}\n",
    "predictions_dev = {}\n",
    "\n",
    "# cambia las predicciones del tipo <ftlabel><C> a simplemente <C>\n",
    "for model_name in model_names:\n",
    "    predictions_dev[model_name] = {}\n",
    "    predictions[model_name] = {}\n",
    "    for tema in temas:\n",
    "        predictions[model_name][tema] = []\n",
    "        for pred_list_labels in predictions_with_labels[model_name][tema]:\n",
    "            pred_list = []\n",
    "            for label in pred_list_labels:\n",
    "                pred_list.append(int(label[len(ftlabel):]))\n",
    "            predictions[model_name][tema].append(pred_list)\n",
    "        predictions_dev[model_name][tema] = []\n",
    "        for pred_list_labels in predictions_dev_with_labels[model_name][tema]:\n",
    "            pred_list = []\n",
    "            for label in pred_list_labels:\n",
    "                pred_list.append(int(label[len(ftlabel):]))\n",
    "            predictions_dev[model_name][tema].append(pred_list)\n",
    "\n",
    "            \n",
    "# utility function to select the first prediction from a list of predictions and generate a 1D list of single predictions\n",
    "def first_prediction(lists_of_predictions):\n",
    "    out = []\n",
    "    for predictions in lists_of_predictions:\n",
    "        out.append(predictions[0])\n",
    "    return out      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera reportes para primera predicciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tema 1\t\tacc\trec\tprec\tf1\n",
      "devs-nptv\t65.19\t65.19\t64.08\t64.12\n",
      "test-nptv\t65.89\t65.89\t65.06\t64.9\n",
      "devs-ptv\t67.34\t67.34\t66.26\t66.39\n",
      "test-ptv\t67.1\t67.1\t66.24\t66.24\n",
      "\n",
      "Tema 2\t\tacc\trec\tprec\tf1\n",
      "devs-nptv\t71.84\t71.84\t71.05\t71.04\n",
      "test-nptv\t68.56\t68.56\t67.95\t67.89\n",
      "devs-ptv\t72.27\t72.27\t71.72\t71.66\n",
      "test-ptv\t70.78\t70.78\t70.49\t70.32\n",
      "\n",
      "Tema 3\t\tacc\trec\tprec\tf1\n",
      "devs-nptv\t75.85\t75.85\t75.97\t75.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jperez/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/jperez/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-nptv\t75.08\t75.08\t75.14\t74.86\n",
      "devs-ptv\t76.38\t76.38\t76.43\t76.23\n",
      "test-ptv\t75.73\t75.73\t75.72\t75.63\n",
      "\n",
      "Tema 4\t\tacc\trec\tprec\tf1\n",
      "devs-nptv\t69.97\t69.97\t68.96\t69.06\n",
      "test-nptv\t68.45\t68.45\t67.91\t67.84\n",
      "devs-ptv\t70.84\t70.84\t69.95\t70.06\n",
      "test-ptv\t69.34\t69.34\t68.89\t68.83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for i in temas:\n",
    "    print(\"Tema \" + str(i) + \"\\t\\tacc\\trec\\tprec\\tf1\")\n",
    "    for model,model_name in zip(['_model_at_1','_model_ptvec_at_1'],['nptv','ptv']):\n",
    "        for pred,gold,set_name in zip(\n",
    "            [predictions_dev[model][i],predictions[model][i]],\n",
    "            [dev_y[i],test_y[i]],\n",
    "            ['devs','test']):\n",
    "            prediction = first_prediction(pred)\n",
    "            acc = round(100*metrics.accuracy_score(gold,prediction),2)\n",
    "            rec = round(100*metrics.recall_score(gold,prediction,average='weighted'),2)\n",
    "            prec = round(100*metrics.precision_score(gold,prediction,average='weighted'),2)\n",
    "            f1 = round(100*metrics.f1_score(gold,prediction,average='weighted'),2)\n",
    "            print(set_name + \"-\" + model_name + \"\\t\"+ str(acc)+ \"\\t\"+ str(rec) +\"\\t\"+str(prec)+\"\\t\"+str(f1))\n",
    "            #print(metrics.classification_report(test_y[i],prediction))\n",
    "    print()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera reporte para predicciones en listas (top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to compute top k accuracy\n",
    "\n",
    "def top_k_accuracy(gold,predicted,k):\n",
    "    '''\n",
    "    #Arguments\n",
    "        gold: the true labels of the test cases (size N = number of test cases)\n",
    "        predicted: ranked list of label predictions for every test case (size N x L, where L is assumed to be >= k)\n",
    "        k: the number of elements in the predicted lists that should be considered to compute the metric\n",
    "    #Returns\n",
    "        The portion of cases (between 0 and 1) in which the true label value was among the first k predicted labels\n",
    "    '''\n",
    "    count = 0\n",
    "    for g,pred_labels in zip(gold,predicted):\n",
    "        if g in pred_labels[:k]:\n",
    "                count += 1\n",
    "    return count/len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tema 1\t\t3\t5\t7\t10\n",
      "devs-ptv\t85.34\t90.76\t93.3\t95.45\n",
      "test-ptv\t85.64\t90.72\t93.26\t95.58\n",
      "devs-npt\t84.23\t89.66\t91.91\t94.47\n",
      "test-npt\t84.17\t89.44\t92.3\t94.53\n",
      "\n",
      "Tema 2\t\t3\t5\t7\t10\n",
      "devs-ptv\t88.86\t92.68\t94.1\t95.7\n",
      "test-ptv\t88.44\t92.33\t94.6\t95.95\n",
      "devs-npt\t87.69\t91.9\t93.5\t94.99\n",
      "test-npt\t86.52\t90.62\t93.16\t94.65\n",
      "\n",
      "Tema 3\t\t3\t5\t7\t10\n",
      "devs-ptv\t92.25\t96.61\t97.73\t99.1\n",
      "test-ptv\t92.37\t96.36\t98.03\t99.35\n",
      "devs-npt\t91.63\t95.94\t97.53\t99.08\n",
      "test-npt\t91.28\t95.81\t97.78\t99.38\n",
      "\n",
      "Tema 4\t\t3\t5\t7\t10\n",
      "devs-ptv\t87.35\t92.24\t94.53\t96.69\n",
      "test-ptv\t87.38\t92.52\t94.76\t97.0\n",
      "devs-npt\t86.49\t91.68\t93.89\t96.21\n",
      "test-npt\t86.62\t91.15\t93.89\t96.16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_options = [3,5,7,10]\n",
    "for i in temas:\n",
    "    head_str = \"Tema \" + str(i) + \"\\t\\t\"\n",
    "    head_str += \"\\t\".join([str(op) for op in top_k_options])\n",
    "    print(head_str)\n",
    "    for model, model_name in zip(['_model_ptvec_at_5','_model_at_5'],['ptv','npt']):\n",
    "        for prediction,gold,set_name in zip(\n",
    "                [predictions_dev[model][i],predictions[model][i]],\n",
    "                [dev_y[i],test_y[i]],\n",
    "                ['devs','test']):\n",
    "            data_str = set_name + \"-\" + model_name\n",
    "            for k in top_k_options:\n",
    "                top_k = round(100*top_k_accuracy(gold,prediction,k),2)\n",
    "                data_str += \"\\t\" + str(top_k)\n",
    "            print(data_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ranking_sizes(gold,predicted):\n",
    "    r_sizes = []\n",
    "    for g,pred_labels in zip(gold,predicted):\n",
    "        if g not in pred_labels:\n",
    "            raise Exception('Label ' + str(g) + ' is not in the ranking.')\n",
    "        r_sizes.append(pred_labels.index(g) + 1)\n",
    "    return np.array(r_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tema 1\t\t80%\t85%\t90%\t95%\n",
      "devs-ptv\t2\t3\t5\t10\n",
      "test-ptv\t2\t3\t5\t10\n",
      "devs-npt\t3\t4\t6\t11\n",
      "test-npt\t3\t4\t6\t11\n",
      "\n",
      "Tema 2\t\t80%\t85%\t90%\t95%\n",
      "devs-ptv\t2\t3\t4\t9\n",
      "test-ptv\t2\t3\t4\t8\n",
      "devs-npt\t2\t3\t4\t10\n",
      "test-npt\t2\t3\t5\t12\n",
      "\n",
      "Tema 3\t\t80%\t85%\t90%\t95%\n",
      "devs-ptv\t2\t2\t3\t4\n",
      "test-ptv\t2\t2\t3\t5\n",
      "devs-npt\t2\t2\t3\t5\n",
      "test-npt\t2\t2\t3\t5\n",
      "\n",
      "Tema 4\t\t80%\t85%\t90%\t95%\n",
      "devs-ptv\t2\t3\t5\t8\n",
      "test-ptv\t2\t3\t4\t8\n",
      "devs-npt\t2\t3\t5\t9\n",
      "test-npt\t2\t3\t5\t9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentile_options = [80,85,90,95]\n",
    "for i in temas:\n",
    "    head_str = \"Tema \" + str(i) + \"\\t\\t\"\n",
    "    head_str += \"\\t\".join([str(op)+\"%\" for op in percentile_options])\n",
    "    print(head_str)\n",
    "    for model, model_name in zip(['_model_ptvec_at_5','_model_at_5'],['ptv','npt']):\n",
    "        for prediction,gold,set_name in zip(\n",
    "                [predictions_dev[model][i],predictions[model][i]],\n",
    "                [dev_y[i],test_y[i]],\n",
    "                ['devs','test']):\n",
    "            data_str = set_name + \"-\" + model_name\n",
    "            for k in percentile_options:\n",
    "                percentile = int(np.percentile(ranking_sizes(gold,prediction),k))\n",
    "                data_str += \"\\t\" + str(percentile)\n",
    "            print(data_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
